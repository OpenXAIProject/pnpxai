{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cwl/miniconda3/envs/pnpenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/cwl/miniconda3/envs/pnpenv/lib/python3.10/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "import optuna\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9514ee9e70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed data\n",
    "X_train = np.load(\"data/baf/preprocess/X_train.npy\")\n",
    "y_train = np.load(\"data/baf/preprocess/y_train.npy\")\n",
    "X_valid = np.load(\"data/baf/preprocess/X_valid.npy\")\n",
    "y_valid = np.load(\"data/baf/preprocess/y_valid.npy\")\n",
    "\n",
    "with open(\"data/baf/preprocess/metadata.pkl\", 'rb') as f:\n",
    "    metadata = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.95      0.91      5000\n",
      "           1       0.73      0.47      0.58      1411\n",
      "\n",
      "    accuracy                           0.85      6411\n",
      "   macro avg       0.80      0.71      0.74      6411\n",
      "weighted avg       0.84      0.85      0.83      6411\n",
      "\n",
      "ROC-AUC: 0.8752963855421687\n",
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.95      0.91      5000\n",
      "           1       0.75      0.49      0.59      1411\n",
      "\n",
      "    accuracy                           0.85      6411\n",
      "   macro avg       0.81      0.72      0.75      6411\n",
      "weighted avg       0.84      0.85      0.84      6411\n",
      "\n",
      "ROC-AUC: 0.8760414599574772\n",
      "XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91      5000\n",
      "           1       0.73      0.56      0.63      1411\n",
      "\n",
      "    accuracy                           0.86      6411\n",
      "   macro avg       0.81      0.75      0.77      6411\n",
      "weighted avg       0.85      0.86      0.85      6411\n",
      "\n",
      "ROC-AUC: 0.8944106307583274\n"
     ]
    }
   ],
   "source": [
    "# Train a logistic regression model\n",
    "model_lr = LogisticRegression(max_iter=1000)\n",
    "model_lr.fit(X_train, y_train)\n",
    "\n",
    "# Train a random forest model\n",
    "model_rf = RandomForestClassifier(n_estimators=100)\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "# Train a xgboost model\n",
    "model_xgb = xgb.XGBClassifier(max_depth=3, n_estimators=100)\n",
    "model_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the models\n",
    "y_pred = model_lr.predict(X_valid)\n",
    "print(\"Logistic Regression\")\n",
    "print(classification_report(y_valid, y_pred))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_valid, model_lr.predict_proba(X_valid)[:, 1]))\n",
    "\n",
    "y_pred = model_rf.predict(X_valid)\n",
    "print(\"Random Forest\")\n",
    "print(classification_report(y_valid, y_pred))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_valid, model_rf.predict_proba(X_valid)[:, 1]))\n",
    "\n",
    "y_pred = model_xgb.predict(X_valid)\n",
    "print(\"XGBoost\")\n",
    "print(classification_report(y_valid, y_pred))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_valid, model_xgb.predict_proba(X_valid)[:, 1]))\n",
    "\n",
    "\n",
    "# Save models\n",
    "with open(\"models/baf/model_lr.pkl\", 'wb') as f:\n",
    "    pickle.dump(model_lr, f)\n",
    "\n",
    "with open(\"models/baf/model_rf.pkl\", 'wb') as f:\n",
    "    pickle.dump(model_rf, f)\n",
    "\n",
    "with open(\"models/baf/model_xgb.pkl\", 'wb') as f:\n",
    "    pickle.dump(model_xgb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.bn = torch.nn.BatchNorm1d(in_features)\n",
    "        self.fc1 = torch.nn.Linear(in_features, out_features)\n",
    "        self.fc2 = torch.nn.Linear(out_features, out_features)\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = torch.relu(self.fc1(self.bn(x)))\n",
    "        y = self.dropout(y)\n",
    "        y = self.fc2(y)\n",
    "        y = self.dropout(y)\n",
    "        return torch.add(x, y)\n",
    "    \n",
    "class TabResNet(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_blocks=1, embedding_dim=128):\n",
    "        super(TabResNet, self).__init__()\n",
    "        self.embedding = torch.nn.Linear(in_features, embedding_dim)\n",
    "        self.res_blocks = []\n",
    "        for i in range(num_blocks):\n",
    "            self.res_blocks.append(ResNetBlock(embedding_dim, embedding_dim))\n",
    "        self.res_blocks = torch.nn.ModuleList(self.res_blocks)\n",
    "        self.bn = torch.nn.BatchNorm1d(embedding_dim)\n",
    "        self.fc = torch.nn.Linear(embedding_dim, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        for block in self.res_blocks:\n",
    "            x = block(x)\n",
    "        x = torch.relu(self.bn(x))\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.3864909635420051\n",
      "Validation Loss: 0.3482488691806793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.91      0.90      5000\n",
      "           1       0.66      0.63      0.64      1411\n",
      "\n",
      "    accuracy                           0.85      6411\n",
      "   macro avg       0.78      0.77      0.77      6411\n",
      "weighted avg       0.84      0.85      0.84      6411\n",
      "\n",
      "ROC-AUC: 0.8750987243090008\n",
      "Epoch 1, Loss: 0.3666435921092353\n",
      "Validation Loss: 0.3406158983707428\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91      5000\n",
      "           1       0.72      0.55      0.62      1411\n",
      "\n",
      "    accuracy                           0.85      6411\n",
      "   macro avg       0.80      0.75      0.77      6411\n",
      "weighted avg       0.85      0.85      0.85      6411\n",
      "\n",
      "ROC-AUC: 0.8748755492558469\n",
      "Epoch 2, Loss: 0.3628064251758836\n",
      "Validation Loss: 0.3438488245010376\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.90      5000\n",
      "           1       0.67      0.60      0.64      1411\n",
      "\n",
      "    accuracy                           0.85      6411\n",
      "   macro avg       0.78      0.76      0.77      6411\n",
      "weighted avg       0.84      0.85      0.85      6411\n",
      "\n",
      "ROC-AUC: 0.8746155917788802\n",
      "Epoch 3, Loss: 0.3603441029359279\n",
      "Validation Loss: 0.3407426178455353\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.91      5000\n",
      "           1       0.70      0.56      0.62      1411\n",
      "\n",
      "    accuracy                           0.85      6411\n",
      "   macro avg       0.79      0.75      0.76      6411\n",
      "weighted avg       0.84      0.85      0.84      6411\n",
      "\n",
      "ROC-AUC: 0.8724540042523034\n",
      "Epoch 4, Loss: 0.35595341436743166\n",
      "Validation Loss: 0.3378458321094513\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91      5000\n",
      "           1       0.71      0.57      0.63      1411\n",
      "\n",
      "    accuracy                           0.85      6411\n",
      "   macro avg       0.80      0.75      0.77      6411\n",
      "weighted avg       0.85      0.85      0.85      6411\n",
      "\n",
      "ROC-AUC: 0.8759892274982283\n",
      "Epoch 5, Loss: 0.35344515502025065\n",
      "Validation Loss: 0.34195467829704285\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.91      5000\n",
      "           1       0.69      0.60      0.64      1411\n",
      "\n",
      "    accuracy                           0.85      6411\n",
      "   macro avg       0.79      0.76      0.78      6411\n",
      "weighted avg       0.85      0.85      0.85      6411\n",
      "\n",
      "ROC-AUC: 0.8791288447909285\n",
      "Epoch 6, Loss: 0.35123011669473786\n",
      "Validation Loss: 0.34575751423835754\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.91      0.90      5000\n",
      "           1       0.66      0.65      0.66      1411\n",
      "\n",
      "    accuracy                           0.85      6411\n",
      "   macro avg       0.78      0.78      0.78      6411\n",
      "weighted avg       0.85      0.85      0.85      6411\n",
      "\n",
      "ROC-AUC: 0.8840980864635011\n",
      "Epoch 7, Loss: 0.3516441791821895\n",
      "Validation Loss: 0.33680492639541626\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91      5000\n",
      "           1       0.70      0.59      0.64      1411\n",
      "\n",
      "    accuracy                           0.85      6411\n",
      "   macro avg       0.80      0.76      0.77      6411\n",
      "weighted avg       0.85      0.85      0.85      6411\n",
      "\n",
      "ROC-AUC: 0.8811706591070162\n",
      "Epoch 8, Loss: 0.3471688474550772\n",
      "Validation Loss: 0.34072986245155334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.91      5000\n",
      "           1       0.70      0.56      0.62      1411\n",
      "\n",
      "    accuracy                           0.85      6411\n",
      "   macro avg       0.79      0.75      0.76      6411\n",
      "weighted avg       0.84      0.85      0.84      6411\n",
      "\n",
      "ROC-AUC: 0.8794595322466335\n",
      "Epoch 9, Loss: 0.34539957640416313\n",
      "Validation Loss: 0.3431777060031891\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91      5000\n",
      "           1       0.72      0.55      0.62      1411\n",
      "\n",
      "    accuracy                           0.85      6411\n",
      "   macro avg       0.80      0.74      0.77      6411\n",
      "weighted avg       0.84      0.85      0.85      6411\n",
      "\n",
      "ROC-AUC: 0.8750483345145288\n"
     ]
    }
   ],
   "source": [
    "model = TabResNet(X_train.shape[1], 2)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_valid_tensor = torch.tensor(X_valid, dtype=torch.float32)\n",
    "y_valid_tensor = torch.tensor(y_valid, dtype=torch.long)\n",
    "\n",
    "# Create a DataLoader\n",
    "train_data = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.train()\n",
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, (X_batch, y_batch) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}, Loss: {train_loss / len(train_loader)}\")\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_valid_tensor)\n",
    "        loss = criterion(y_pred, y_valid_tensor)\n",
    "        valid_loss = loss.item()\n",
    "\n",
    "        print(f\"Validation Loss: {valid_loss}\")\n",
    "        y_pred = torch.argmax(y_pred, dim=1)\n",
    "        print(classification_report(y_valid_tensor, y_pred))\n",
    "        print(\"ROC-AUC:\", roc_auc_score(y_valid, model(X_valid_tensor).detach().numpy()[:, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models/baf/tabresnet.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STOP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mSTOP\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'STOP' is not defined"
     ]
    }
   ],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna_studies = []\n",
    "\n",
    "for model_name in [\"logistic_regression\", \"random_forest\", \"xgboost\"]:\n",
    "    def generate_objective(X_train, y_train, cv=3, scoring=\"roc_auc\"):\n",
    "        def objective(trial):\n",
    "            if model_name == \"logistic_regression\":\n",
    "                params = {\n",
    "                    'C': trial.suggest_float('C', 1e-3, 10.0, log=True),\n",
    "                    'solver': trial.suggest_categorical('solver', ['liblinear', 'lbfgs']),\n",
    "                    'max_iter': 1000\n",
    "                }\n",
    "                model = LogisticRegression(**params)\n",
    "            elif model_name == \"random_forest\":\n",
    "                params = {\n",
    "                    'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 2, 20),\n",
    "                    'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "                    'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "                    'bootstrap': trial.suggest_categorical('bootstrap', [True, False])\n",
    "                }\n",
    "                model = RandomForestClassifier(**params)\n",
    "            elif model_name == \"xgboost\":\n",
    "                params = {\n",
    "                    'verbosity': 0,\n",
    "                    'objective': 'binary:logistic',\n",
    "                    'booster': 'gbtree',\n",
    "                    'lambda': trial.suggest_float('lambda', 1e-3, 10.0, log=True),\n",
    "                    'alpha': trial.suggest_float('alpha', 1e-3, 10.0, log=True),\n",
    "                    'subsample': trial.suggest_uniform('subsample', 0.4, 1.0),\n",
    "                    'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.4, 1.0),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 2, 10),\n",
    "                    'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "                    'eta': trial.suggest_float('eta', 1e-3, 0.1, log=True),\n",
    "                    'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "                    'gamma': trial.suggest_float('gamma', 1e-3, 10.0, log=True),\n",
    "                }\n",
    "                model = xgb.XGBClassifier(**params)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid model name\")\n",
    "\n",
    "            scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=scoring)\n",
    "            return scores.mean()\n",
    "\n",
    "        return objective\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(generate_objective(X_train, y_train), n_trials=100)\n",
    "\n",
    "    if model_name == \"logistic_regression\":\n",
    "        best_model = LogisticRegression(**study.best_params)\n",
    "    elif model_name == \"random_forest\":\n",
    "        best_model = RandomForestClassifier(**study.best_params)\n",
    "    elif model_name == \"xgboost\":\n",
    "        best_model = xgb.XGBClassifier(**study.best_params)\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    res = {\n",
    "        \"model_name\": model_name,\n",
    "        \"best_model\" : best_model,\n",
    "        \"best_params\": study.best_params,\n",
    "        \"best_score\": study.best_value,\n",
    "    }\n",
    "\n",
    "    optuna_studies.append(res)\n",
    "\n",
    "    y_pred = best_model.predict(X_valid)\n",
    "    print(classification_report(y_valid, y_pred))\n",
    "    print(roc_auc_score(y_valid, best_model.predict_proba(X_valid)[:, 1]))\n",
    "\n",
    "\n",
    "with open(\"data/baf/optuna_studies.pkl\", 'wb') as f:\n",
    "    pickle.dump(optuna_studies, f)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pnpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
