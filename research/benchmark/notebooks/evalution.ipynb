{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Tabular 데이터에 대해 AbPC 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cwl/miniconda3/envs/pnpenv/lib/python3.10/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import functools\n",
    "from math import comb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transform(X, feature_metadata):\n",
    "    input_data = []\n",
    "    for k, v in feature_metadata.items():\n",
    "        preprocessed = v['encoder'].transform(X[[k]].values)\n",
    "        if v['type'] == 'categorical':\n",
    "            preprocessed = preprocessed.toarray()\n",
    "        input_data.append(preprocessed)\n",
    "    \n",
    "    input_array = np.concatenate(input_data, axis=1)\n",
    "    return input_array\n",
    "\n",
    "def _invert_input_array(input_array, feature_metadata):\n",
    "    inverted_data = {}\n",
    "    \n",
    "    for col, meta in feature_metadata.items():\n",
    "        if meta['type'] == 'categorical':\n",
    "            # One-hot encoded 된 부분 추출\n",
    "            start_idx, end_idx = meta['index'][0], meta['index'][-1] + 1\n",
    "            cat_data = input_array[:, start_idx:end_idx]\n",
    "            # OneHotEncoder로 복원\n",
    "            inverted_col = meta['encoder'].inverse_transform(cat_data)\n",
    "            inverted_data[col] = inverted_col.flatten()\n",
    "        else:\n",
    "            # 수치형 데이터 복원\n",
    "            idx = meta['index']\n",
    "            num_data = input_array[:, idx].reshape(-1, 1)\n",
    "            inverted_col = meta['encoder'].inverse_transform(num_data)\n",
    "            inverted_data[col] = inverted_col.flatten()\n",
    "    \n",
    "    # 복원된 데이터를 DataFrame으로 변환\n",
    "    inverted_df = pd.DataFrame(inverted_data)\n",
    "    \n",
    "    return inverted_df\n",
    "\n",
    "\n",
    "def find_closest_data_with_center(X_train, cluster_centers):\n",
    "    closest_data = []\n",
    "    \n",
    "    for center in cluster_centers:\n",
    "        # 각 중심에 대해 유클리드 거리 계산\n",
    "        distances = np.linalg.norm(X_train - center, axis=1)\n",
    "        # 가장 가까운 데이터의 인덱스 찾기\n",
    "        closest_index = np.argmin(distances)\n",
    "        # 가장 가까운 데이터 추가\n",
    "        closest_data.append(X_train[closest_index])\n",
    "    \n",
    "    return np.array(closest_data)\n",
    "\n",
    "def shapley_kernel(N):\n",
    "    kernel = np.zeros(N)\n",
    "    for s in range(1, N):  # subset 크기 s는 1부터 N-1까지\n",
    "        kernel[s] = (N - 1) / (comb(N, s) * s * (N - s))\n",
    "    return kernel / kernel.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_nm = \"Adult\"\n",
    "X_train = np.load(f\"data/{dataset_nm}/X_train.npy\")\n",
    "y_train = np.load(f\"data/{dataset_nm}/y_train.npy\")\n",
    "\n",
    "X_test = np.load(f\"data/{dataset_nm}/X_test.npy\")\n",
    "y_test = np.load(f\"data/{dataset_nm}/y_test.npy\")\n",
    "\n",
    "feature_metadata = pickle.load(open(f\"data/{dataset_nm}/feature_metadata.pkl\", \"rb\"))\n",
    "invert_input_array = functools.partial(_invert_input_array, feature_metadata=feature_metadata)\n",
    "transform = functools.partial(_transform, feature_metadata=feature_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = []\n",
    "cat_cols = []\n",
    "\n",
    "for i, (col_nm, info) in enumerate(feature_metadata.items()):\n",
    "    if info['type'] == 'numerical':\n",
    "        num_cols.append(col_nm)\n",
    "    else:\n",
    "        cat_cols.append(col_nm)\n",
    "        encoder = info['encoder']\n",
    "        weight = info['cat_dist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=50, random_state=42)\n",
    "kmeans.fit(X_train)\n",
    "\n",
    "res = kmeans.predict(X_train)\n",
    "_, counts = np.unique(res, return_counts=True)\n",
    "weight = counts / counts.sum()\n",
    "\n",
    "bg_data = find_closest_data_with_center(X_train, kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_data = invert_input_array(bg_data)\n",
    "train_data = invert_input_array(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier()\n",
    "model.load_model(f\"data/{dataset_nm}/xgb_model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = 10\n",
    "n_coalition = 100\n",
    "orig_feature = bg_data.columns.copy()\n",
    "\n",
    "target = invert_input_array(X_test[:target_size])\n",
    "base_y = model.predict_proba(transform(target))[:, 1]\n",
    "\n",
    "record = {\"bin_coal\" : [], \"perturbed\" : []}\n",
    "for _ in range(n_coalition):\n",
    "    coalition_sz = np.random.choice(np.arange(len(feature_metadata)), replace=False, p=shapley_kernel(len(feature_metadata)))\n",
    "    coalition = np.random.permutation(len(feature_metadata))[:coalition_sz]\n",
    "\n",
    "    coal_feature = orig_feature[coalition]\n",
    "    non_coal_feature = [col for col in orig_feature if col not in coal_feature]\n",
    "\n",
    "    coal_data = target.loc[target.index.repeat(len(bg_data)), coal_feature].reset_index(drop=True)\n",
    "    non_coal_data = bg_data.loc[np.tile(bg_data.index, len(target))].reset_index(drop=True)[non_coal_feature]\n",
    "    new_data = pd.concat([coal_data, non_coal_data], axis=1)[orig_feature]\n",
    "\n",
    "    input_data = transform(new_data)\n",
    "    pred = model.predict_proba(input_data)[:, 1]\n",
    "\n",
    "    perturbed = pred.reshape(-1, bg_data.shape[0]) @ weight\n",
    "    \n",
    "    bin_coal = np.zeros(len(feature_metadata), dtype=int)\n",
    "    bin_coal[coalition] = 1\n",
    "    \n",
    "    record[\"perturbed\"].append(perturbed)\n",
    "    record[\"bin_coal\"].append(bin_coal)\n",
    "\n",
    "record[\"perturbed\"] = np.array(record[\"perturbed\"])\n",
    "record[\"bin_coal\"] = np.array(record[\"bin_coal\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximize_correlation(X, y, epsilon=1e-6):\n",
    "    n, m = X.shape\n",
    "    \n",
    "    # 평균 중심화\n",
    "    X_centered = X - X.mean(axis=0)\n",
    "    y_centered = y - y.mean()\n",
    "    \n",
    "    # 공분산 행렬 생성\n",
    "    cov_matrix = X_centered.T @ X_centered / n\n",
    "    \n",
    "    # 정규화 항 추가 (ridge regression 방식)\n",
    "    regularized_cov = cov_matrix + epsilon * np.eye(m)\n",
    "    \n",
    "    # beta 추정값 계산\n",
    "    beta = np.linalg.solve(regularized_cov, X_centered.T @ y_centered / n)\n",
    "    \n",
    "    # 상관계수 최대화를 위해 beta의 크기를 스케일링\n",
    "    beta /= np.linalg.norm(beta)\n",
    "    \n",
    "    return beta\n",
    "\n",
    "\n",
    "res = maximize_correlation(record['bin_coal'], record['perturbed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 14), (100, 10))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record['bin_coal'].shape, record['perturbed'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.8035243e-03, 4.8375152e-02, 3.5502409e-04, 7.0616817e-03,\n",
       "        6.7211664e-01, 1.8720534e-02, 9.6442008e-01, 2.4355875e-02,\n",
       "        2.2555871e-01, 2.1727468e-01], dtype=float32),\n",
       " array([False, False, False, False,  True, False,  True, False, False,\n",
       "        False]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_y, base_y > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_masked_vector(explanation, d):\n",
    "    \"\"\"\n",
    "    Create a masked binary vector for MORF or LERF calculation.\n",
    "    \"\"\"\n",
    "    sorted_idx = np.argsort(-explanation)  # 내림차순 정렬\n",
    "    bin_vec = np.ones((d + 1, d), dtype=int)\n",
    "    row_indices = np.broadcast_to(np.arange(d + 1)[:, None], (d + 1, d))\n",
    "    sorted_indices = np.broadcast_to(sorted_idx, (d + 1, d))\n",
    "    mask = (np.arange(d)[None, :] < np.arange(d + 1)[:, None])\n",
    "    bin_vec[row_indices[mask], sorted_indices[mask]] = 0\n",
    "    return bin_vec\n",
    "\n",
    "def compute_curve(bin_vec, target, bg_data, model, transform, pred_label, weight):\n",
    "    \"\"\"\n",
    "    Compute MORF or LERF curve based on the masked vector.\n",
    "    \"\"\"\n",
    "    orig_feature = bg_data.columns.copy()\n",
    "    curve = np.zeros(bin_vec.shape[0])\n",
    "    \n",
    "    for i, bin_coal in enumerate(bin_vec):\n",
    "        coalition = np.where(bin_coal == 1)[0]\n",
    "        coal_feature = orig_feature[coalition]\n",
    "        non_coal_feature = [col for col in orig_feature if col not in coal_feature]\n",
    "\n",
    "        coal_data = target.loc[target.index.repeat(len(bg_data)), coal_feature].reset_index(drop=True)\n",
    "        non_coal_data = bg_data.loc[np.tile(bg_data.index, len(target))].reset_index(drop=True)[non_coal_feature]\n",
    "        new_data = pd.concat([coal_data, non_coal_data], axis=1)[orig_feature]\n",
    "\n",
    "        input_data = transform(new_data)\n",
    "        pred = model.predict_proba(input_data)[:, pred_label]\n",
    "\n",
    "        # 곡선 업데이트\n",
    "        curve[i] = pred.reshape(-1, bg_data.shape[0]) @ weight\n",
    "\n",
    "    return curve\n",
    "\n",
    "def compute_abpc(X_test, bg_data, model, transform, explanation, weight):\n",
    "    \"\"\"\n",
    "    Compute ABPC based on MORF and LERF curves.\n",
    "    \"\"\"\n",
    "    target = invert_input_array(X_test)\n",
    "    proba = model.predict_proba(transform(target))\n",
    "    pred_label = proba.argmax()\n",
    "    print(pred_label)\n",
    "\n",
    "    # MORF 곡선 생성\n",
    "    morf_bin_vec = create_masked_vector(explanation, len(bg_data.columns))\n",
    "    morf_curve = compute_curve(morf_bin_vec, target, bg_data, model, transform, pred_label, weight)\n",
    "\n",
    "    # LERF 곡선 생성\n",
    "    lerf_bin_vec = create_masked_vector(-explanation, len(bg_data.columns))\n",
    "    lerf_curve = compute_curve(lerf_bin_vec, target, bg_data, model, transform, pred_label, weight)\n",
    "\n",
    "    if pred_label == 0:\n",
    "        morf_curve, lerf_curve = lerf_curve, morf_curve\n",
    "\n",
    "    # ABPC 계산\n",
    "    abpc = (lerf_curve - morf_curve).mean()\n",
    "    return abpc\n",
    "\n",
    "# 실행 예제\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "ABPC: 0.1735\n",
      "0\n",
      "ABPC: 0.2419\n",
      "0\n",
      "ABPC: 0.1706\n",
      "0\n",
      "ABPC: 0.2825\n",
      "1\n",
      "ABPC: 0.3220\n",
      "0\n",
      "ABPC: 0.1822\n",
      "1\n",
      "ABPC: 0.4171\n",
      "0\n",
      "ABPC: 0.1921\n",
      "0\n",
      "ABPC: 0.2898\n",
      "0\n",
      "ABPC: 0.2680\n"
     ]
    }
   ],
   "source": [
    "for i in range(res.shape[1]):\n",
    "    explanation = res[:,i]\n",
    "    abpc = compute_abpc(X_test[[i]], bg_data, model, transform, explanation, weight)\n",
    "    print(f\"ABPC: {abpc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 10)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pnpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
