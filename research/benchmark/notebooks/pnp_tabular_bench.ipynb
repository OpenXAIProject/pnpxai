{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다양한 framework에서의 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import pickle\n",
    "# import functools\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import xgboost as xgb\n",
    "\n",
    "# from ucimlrepo import fetch_ucirepo\n",
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoXAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"lib/AutoXAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
      "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
      "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
      "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
      "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
      "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
      "\n",
      "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
      "0        3.06                  0.28             2.29             5.64  1.04   \n",
      "1        2.76                  0.26             1.28             4.38  1.05   \n",
      "2        3.24                  0.30             2.81             5.68  1.03   \n",
      "3        3.49                  0.24             2.18             7.80  0.86   \n",
      "4        2.69                  0.39             1.82             4.32  1.04   \n",
      "\n",
      "   od280/od315_of_diluted_wines  proline  \n",
      "0                          3.92   1065.0  \n",
      "1                          3.40   1050.0  \n",
      "2                          3.17   1185.0  \n",
      "3                          3.45   1480.0  \n",
      "4                          2.93    735.0  \n",
      "0    False\n",
      "1    False\n",
      "2    False\n",
      "3    False\n",
      "4    False\n",
      "Name: target, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes, load_wine\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터 불러오기\n",
    "wine = load_wine()\n",
    "\n",
    "# pandas DataFrame으로 변환\n",
    "X = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "y = pd.Series(wine.target, name='target') > 6\n",
    "\n",
    "# 데이터 확인\n",
    "print(X.head())\n",
    "print(y.head())\n",
    "\n",
    "data = pd.concat([X, y], axis=1)\n",
    "data.to_csv(\"wine.csv\", index=False)\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pickle\n",
    "# MLPRegressor 모델 생성\n",
    "model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n",
    "# 모델 학습\n",
    "model.fit(X, y)\n",
    "# 모델 저장\n",
    "with open(\"mlp_diabetes_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "X = X.values\n",
    "y = y.values\n",
    "feature_names = wine.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cwl/miniconda3/envs/autoxai/lib/python3.8/site-packages/shap/utils/_clustering.py:35: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _pt_shuffle_rec(i, indexes, index_mask, partition_tree, M, pos):\n",
      "/home/cwl/miniconda3/envs/autoxai/lib/python3.8/site-packages/shap/utils/_clustering.py:54: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def delta_minimization_order(all_masks, max_swap_size=100, num_passes=2):\n",
      "/home/cwl/miniconda3/envs/autoxai/lib/python3.8/site-packages/shap/utils/_clustering.py:63: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _reverse_window(order, start, length):\n",
      "/home/cwl/miniconda3/envs/autoxai/lib/python3.8/site-packages/shap/utils/_clustering.py:69: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _reverse_window_score_gain(masks, order, start, length):\n",
      "/home/cwl/miniconda3/envs/autoxai/lib/python3.8/site-packages/shap/utils/_clustering.py:77: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _mask_delta_score(m1, m2):\n",
      "/home/cwl/miniconda3/envs/autoxai/lib/python3.8/site-packages/shap/links.py:5: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def identity(x):\n",
      "/home/cwl/miniconda3/envs/autoxai/lib/python3.8/site-packages/shap/links.py:10: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _identity_inverse(x):\n",
      "/home/cwl/miniconda3/envs/autoxai/lib/python3.8/site-packages/shap/links.py:15: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def logit(x):\n",
      "/home/cwl/miniconda3/envs/autoxai/lib/python3.8/site-packages/shap/links.py:20: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _logit_inverse(x):\n",
      "/home/cwl/miniconda3/envs/autoxai/lib/python3.8/site-packages/shap/utils/_masked_model.py:363: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _build_fixed_single_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/home/cwl/miniconda3/envs/autoxai/lib/python3.8/site-packages/shap/utils/_masked_model.py:385: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _build_fixed_multi_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/home/cwl/miniconda3/envs/autoxai/lib/python3.8/site-packages/shap/utils/_masked_model.py:428: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _init_masks(cluster_matrix, M, indices_row_pos, indptr):\n",
      "/home/cwl/miniconda3/envs/autoxai/lib/python3.8/site-packages/shap/utils/_masked_model.py:439: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _rec_fill_masks(cluster_matrix, indices_row_pos, indptr, indices, M, ind):\n",
      "/home/cwl/miniconda3/envs/autoxai/lib/python3.8/site-packages/shap/maskers/_tabular.py:186: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _single_delta_mask(dind, masked_inputs, last_mask, data, x, noop_code):\n",
      "/home/cwl/miniconda3/envs/autoxai/lib/python3.8/site-packages/shap/maskers/_tabular.py:197: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _delta_masking(masks, x, curr_delta_inds, varying_rows_out,\n",
      "/home/cwl/miniconda3/envs/autoxai/lib/python3.8/site-packages/shap/maskers/_image.py:175: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _jit_build_partition_tree(xmin, xmax, ymin, ymax, zmin, zmax, total_ywidth, total_zwidth, M, clustering, q):\n",
      "/home/cwl/miniconda3/envs/autoxai/lib/python3.8/site-packages/shap/explainers/_partition.py:676: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def lower_credit(i, value, M, values, clustering):\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XAI solution: LIME\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 178 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XAI solution: SHAP\n"
     ]
    }
   ],
   "source": [
    "from hyperparameters_optimization import get_parameters, gp_optimization\n",
    "from XAI_solutions import set_up_explainer, get_local_exp\n",
    "\n",
    "\n",
    "context = {}\n",
    "context['model'] = model\n",
    "\n",
    "question = \"Why\"\n",
    "verbose = False\n",
    "task = \"classification\"\n",
    "session_id = 'session-name'\n",
    "scaling = 'Std'\n",
    "weights = [1.0, 2.0, 0.5]\n",
    "distance = \"cosine\"\n",
    "\n",
    "context[\"X\"] = X\n",
    "context[\"y\"] = y\n",
    "context[\"feature_names\"] = feature_names\n",
    "context[\"verbose\"] = verbose\n",
    "context[\"task\"] = task\n",
    "context[\"question\"] = question\n",
    "context[\"session_id\"] = session_id\n",
    "context[\"scaling\"] = scaling\n",
    "context[\"weights\"] = weights\n",
    "context[\"distance\"] = distance\n",
    "context[\"explanations\"] = []\n",
    "evstrat_list = [\"IS\",\"ES\"]\n",
    "\n",
    "if evstrat_list != None:\n",
    "    context[\"ES\"] = True if 'ES' in evstrat_list else False\n",
    "    context[\"IS\"] = True if 'IS' in evstrat_list else False\n",
    "else:\n",
    "    context[\"ES\"] = False\n",
    "    context[\"IS\"] = False\n",
    "\n",
    "score_hist = {}\n",
    "score_hist[\"xai_sol\"] = []\n",
    "score_hist[\"epoch\"] = []\n",
    "score_hist[\"aggregated_score\"] = []\n",
    "score_hist[\"parameters\"] = []\n",
    "\n",
    "properties_list = ['robustness', 'fidelity', 'conciseness']\n",
    "xai_list = [\"LIME\", \"SHAP\"]\n",
    "\n",
    "\n",
    "# TODO to data class property\n",
    "for property in properties_list:\n",
    "    score_hist[property] = []\n",
    "    score_hist[\"scaled_\"+property] = []\n",
    "\n",
    "for xai_sol in xai_list:\n",
    "    print(\"XAI solution:\", xai_sol)\n",
    "    score_hist[\"xai_sol\"].append(xai_sol)\n",
    "    score_hist[\"epoch\"].append(-1)\n",
    "\n",
    "    parameters = get_parameters(\n",
    "        xai_sol, score_hist, \"default\", properties_list, context)\n",
    "    score_hist[\"parameters\"].append(parameters)\n",
    "\n",
    "    n = len(context[\"X\"])\n",
    "    X = context[\"X\"]\n",
    "    exp_values = []\n",
    "    context['explainer'] = set_up_explainer(xai_sol, parameters, context)\n",
    "    for i in range(n):\n",
    "        exp_values += get_local_exp(xai_sol, X[i], parameters, context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STOP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mSTOP\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'STOP' is not defined"
     ]
    }
   ],
   "source": [
    "# python launch.py data/diabetes.csv \"target\" regression -m models/mlp_diabetes_model.pkl -q \"Why\" -p \"robustness,fidelity,conciseness\" --hpo \"gp\" --evstrat \"ES,IS\" -w \"1,2,0.5\" --scaling \"Std\" -s 0 -e 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/autoxai/lib/python3.8/site-packages/pandas/core/indexes/base.py:3361\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3362\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/autoxai/lib/python3.8/site-packages/pandas/_libs/index.pyx:76\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/autoxai/lib/python3.8/site-packages/pandas/_libs/index.pyx:108\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_local_exp(xai_sol, \u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m, parameters, context)\n",
      "File \u001b[0;32m~/miniconda3/envs/autoxai/lib/python3.8/site-packages/pandas/core/frame.py:3458\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3458\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3460\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/autoxai/lib/python3.8/site-packages/pandas/core/indexes/base.py:3363\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3362\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3363\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_scalar(key) \u001b[38;5;129;01mand\u001b[39;00m isna(key) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhasnans:\n\u001b[1;32m   3366\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "get_local_exp(xai_sol, X[i], parameters, context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenXAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from openxai.dataloader import ReturnLoaders\n",
    "from openxai import LoadModel\n",
    "from openxai import Explainer\n",
    "from openxai.experiment_utils import fill_param_dict\n",
    "from openxai.model import ArtificialNeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explainers_dict = {\n",
    "#     'grad': Gradient,\n",
    "#     'sg': SmoothGrad,\n",
    "#     'itg': InputTimesGradient,\n",
    "#     'ig': IntegratedGradients,\n",
    "#     'shap': SHAPExplainerC,\n",
    "#     'lime': LIME,\n",
    "#     'control': RandomBaseline\n",
    "# }\n",
    "\n",
    "# loader_train, loader_test = ReturnLoaders(data_name='german', download=True)\n",
    "# # get an input instance from the test dataset\n",
    "# inputs, labels = next(iter(loader_test))\n",
    "\n",
    "# model = LoadModel(data_name= 'german', ml_model='ann', pretrained=True)\n",
    "# X_train = torch.tensor([d[0] for d in loader_train.dataset])\n",
    "\n",
    "# param_dict = fill_param_dict('lime', {}, X_train)\n",
    "# exp_method = Explainer(method='lime', model=model, param_dict=param_dict)\n",
    "# explanations= exp_method.get_explanations(inputs, label=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_nm = \"Adult\"\n",
    "X_train = np.load(f\"data/{dataset_nm}/X_train.npy\")\n",
    "y_train = np.load(f\"data/{dataset_nm}/y_train.npy\")\n",
    "\n",
    "X_test = np.load(f\"data/{dataset_nm}/X_test.npy\")\n",
    "y_test = np.load(f\"data/{dataset_nm}/y_test.npy\")\n",
    "\n",
    "feature_metadata = pickle.load(open(f\"data/{dataset_nm}/feature_metadata.pkl\", \"rb\"))\n",
    "\n",
    "model = ArtificialNeuralNetwork(input_dim=X_test.shape[1], hidden_layers=[100,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.tensor(X_train, dtype=torch.float32)\n",
    "input_data = torch.tensor(X_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_dict = fill_param_dict('lime', {}, train_data)\n",
    "# exp_method = Explainer(method='lime', model=model, param_dict=param_dict)\n",
    "\n",
    "# exp_method = Explainer(method='grad', model=model)\n",
    "# exp_method = Explainer(method='sg', model=model)\n",
    "# exp_method = Explainer(method='itg', model=model)\n",
    "\n",
    "# param_dict = fill_param_dict('ig', {}, train_data)\n",
    "# exp_method = Explainer(method='ig', model=model, param_dict=param_dict)\n",
    "\n",
    "\n",
    "exp_method = Explainer(method='shap', model=model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 111])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model(input_data).detach().argmax(dim=1)\n",
    "total_exp = []\n",
    "dataset = TensorDataset(input_data, y_pred)\n",
    "loader = DataLoader(dataset, batch_size=10)\n",
    "for inputs, labels in loader:\n",
    "    explanations= exp_method.get_explanations(inputs, label=labels)\n",
    "    total_exp.append(explanations)\n",
    "    break\n",
    "\n",
    "explanations = torch.cat(total_exp, dim=0)\n",
    "explanations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 14])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_metadata\n",
    "\n",
    "_exp = []\n",
    "for nm, info in feature_metadata.items():\n",
    "    if info['type'] == 'categorical':\n",
    "        idx = info['index']\n",
    "        onehot = input_data[:10, idx]\n",
    "        val = explanations[:, idx]\n",
    "        cat_exp = (onehot * val).sum(dim=1)\n",
    "        _exp.append(cat_exp)\n",
    "    else:\n",
    "        _exp.append(explanations[:, info['index']])\n",
    "\n",
    "exp = torch.stack(_exp, dim=1)\n",
    "exp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.7067e-03, -6.2093e-03, -1.7211e-03, -6.9594e-04,  3.8262e-04,\n",
       "          1.6946e-04, -6.6975e-04,  5.7893e-04, -8.1775e-04, -8.0424e-04,\n",
       "          4.6599e-04,  2.4125e-04,  1.9976e-04, -3.9700e-03],\n",
       "        [-4.3620e-03,  3.2987e-03,  1.5954e-03, -1.5827e-03,  1.3526e-03,\n",
       "         -1.8663e-03, -7.3954e-04, -2.4865e-03, -1.1802e-03,  1.9820e-03,\n",
       "         -3.5694e-04, -3.7238e-05,  5.9745e-04, -2.3326e-03],\n",
       "        [ 7.2948e-03,  1.5306e-04, -3.1804e-05,  8.6214e-05, -4.7309e-05,\n",
       "         -4.4448e-04,  1.8585e-03,  2.8437e-03, -3.4996e-04,  1.4028e-03,\n",
       "         -3.5091e-04,  2.0247e-04,  9.6643e-05, -2.3110e-03],\n",
       "        [ 8.5133e-04, -6.5700e-03, -1.1147e-03, -1.2950e-03, -7.8500e-05,\n",
       "          2.2973e-03, -5.1395e-04,  4.6578e-03, -1.3155e-03, -1.7015e-03,\n",
       "          1.0156e-04,  3.6321e-04, -1.1688e-04, -3.4267e-03],\n",
       "        [-4.9224e-04,  1.3795e-03,  8.7882e-04, -8.4938e-04,  1.4198e-04,\n",
       "          8.6840e-04,  1.8120e-03,  3.8192e-03,  1.7027e-03,  9.9984e-04,\n",
       "         -7.3003e-04,  1.1339e-04, -4.6058e-04, -3.1369e-03],\n",
       "        [ 3.4884e-03, -6.6009e-03, -1.0712e-04, -2.0186e-03,  4.3969e-04,\n",
       "          1.1950e-03,  1.4649e-03,  2.1984e-05,  2.8078e-03,  6.6468e-04,\n",
       "         -2.5591e-04,  2.5968e-04,  2.3459e-04, -2.0407e-03],\n",
       "        [-1.0040e-03,  2.7396e-03, -1.4150e-03,  4.0671e-03, -1.6836e-03,\n",
       "          1.0407e-03,  3.6487e-03,  2.7885e-03, -7.7993e-04,  1.5291e-03,\n",
       "         -7.6247e-04,  4.6092e-04, -3.9240e-03, -3.3392e-03],\n",
       "        [ 1.7380e-03, -5.9074e-03,  1.4052e-03, -3.8322e-04, -4.2855e-04,\n",
       "         -2.9504e-04, -1.2715e-03, -2.1535e-03, -9.3990e-04,  1.6524e-03,\n",
       "          1.1854e-05,  6.1930e-05, -5.5800e-05, -2.6152e-03],\n",
       "        [ 1.9445e-03, -6.6321e-03, -3.4607e-04, -9.4780e-04,  4.4004e-04,\n",
       "          2.3160e-03,  3.3592e-04,  3.0815e-03, -4.1009e-04, -1.3385e-03,\n",
       "         -1.7170e-04,  4.9107e-05, -2.2643e-06, -3.9729e-03],\n",
       "        [ 8.0915e-04, -5.3413e-03,  3.8066e-03, -1.8587e-03, -5.0314e-04,\n",
       "          9.6873e-04,  2.2051e-03,  4.6785e-03, -3.9506e-04, -3.6505e-04,\n",
       "         -6.4158e-04,  1.3363e-04,  2.1392e-04, -3.3463e-03]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(openxai.model.ArtificialNeuralNetwork,\n",
       " torch.Tensor,\n",
       " torch.Tensor,\n",
       " torch.Tensor,\n",
       " torch.Size([32, 60]),\n",
       " torch.Size([32]),\n",
       " torch.int64,\n",
       " torch.utils.data.dataloader.DataLoader)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model), type(inputs), type(labels), type(explanations), inputs.shape, labels.shape, labels.dtype, type(loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5909, 0.1146, 1.0000, 0.3333, 0.2000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
       "        1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
       "        0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000], dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OmniXAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 14:48:57.608476: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-13 14:48:59.146516: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741844939.815529   30461 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741844939.977498   30461 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-13 14:49:01.687092: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from omnixai.data.tabular import Tabular\n",
    "from omnixai.preprocessing.tabular import TabularTransform\n",
    "from omnixai.explainers.tabular import TabularExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_nm = \"Adult\"\n",
    "X_train = np.load(f\"data/{dataset_nm}/X_train.npy\")\n",
    "y_train = np.load(f\"data/{dataset_nm}/y_train.npy\")\n",
    "\n",
    "X_test = np.load(f\"data/{dataset_nm}/X_test.npy\")\n",
    "y_test = np.load(f\"data/{dataset_nm}/y_test.npy\")\n",
    "\n",
    "feature_metadata = pickle.load(open(f\"data/{dataset_nm}/feature_metadata.pkl\", \"rb\"))\n",
    "raw_data = pd.read_csv(f\"data/{dataset_nm}/raw_data.csv\")\n",
    "\n",
    "\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.load_model(f\"data/{dataset_nm}/xgb_model.json\")\n",
    "\n",
    "nn_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(X_train.shape[1], 3),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "train_data = Tabular(raw_data, categorical_columns=[c for c in raw_data.columns if feature_metadata[c][\"type\"] == \"categorical\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transform(X, feature_metadata):\n",
    "    input_data = []\n",
    "    for k, v in feature_metadata.items():\n",
    "        preprocessed = v['encoder'].transform(X[[k]].values)\n",
    "        if v['type'] == 'categorical':\n",
    "            preprocessed = preprocessed.toarray()\n",
    "        input_data.append(preprocessed)\n",
    "    \n",
    "    input_array = np.concatenate(input_data, axis=1)\n",
    "    return input_array\n",
    "\n",
    "def _invert_input_array(input_array, feature_metadata):\n",
    "    inverted_data = {}\n",
    "    \n",
    "    for col, meta in feature_metadata.items():\n",
    "        if meta['type'] == 'categorical':\n",
    "            # One-hot encoded 된 부분 추출\n",
    "            start_idx, end_idx = meta['index'][0], meta['index'][-1] + 1\n",
    "            cat_data = input_array[:, start_idx:end_idx]\n",
    "            # OneHotEncoder로 복원\n",
    "            inverted_col = meta['encoder'].inverse_transform(cat_data)\n",
    "            inverted_data[col] = inverted_col.flatten()\n",
    "        else:\n",
    "            # 수치형 데이터 복원\n",
    "            idx = meta['index']\n",
    "            num_data = input_array[:, idx].reshape(-1, 1)\n",
    "            inverted_col = meta['encoder'].inverse_transform(num_data)\n",
    "            inverted_data[col] = inverted_col.flatten()\n",
    "    \n",
    "    # 복원된 데이터를 DataFrame으로 변환\n",
    "    inverted_df = pd.DataFrame(inverted_data)\n",
    "    \n",
    "    return inverted_df\n",
    "        \n",
    "transform = functools.partial(_transform, feature_metadata=feature_metadata)\n",
    "invert_input_array = functools.partial(_invert_input_array, feature_metadata=feature_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = transform(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = TabularExplainer(\n",
    "  explainers=[\"LimeTabular\", \"ShapTabular\", \"lime\", \"shap\"], # The explainers to apply\n",
    "  mode=\"classification\",                             # The task type\n",
    "  data=train_data,\n",
    "  model=xgb_model.predict_proba,                                     # The ML model to explain\n",
    "  preprocess=lambda z: transform(z.data)\n",
    ")\n",
    "\n",
    "nn_explainer = TabularExplainer(\n",
    "  # explainers=[\"LimeTabular\", \"ShapTabular\", \"ig\"], # The explainers to apply\n",
    "  explainers=[\"LimeTabular\", \"ShapTabular\"], # The explainers to apply\n",
    "  mode=\"classification\",                             # The task type\n",
    "  data=train_data,\n",
    "  model=nn_model,                                       # The ML model to explain\n",
    "  preprocess=lambda z: torch.tensor(transform(z.data), dtype=torch.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "396d565204bb409590f5cc6fb5992f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9181e6d68246a7af9352cdda0d6114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1fcbf2d6f4e40678e4ebd61c515f656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_instances = invert_input_array(X_test[:10])\n",
    "\n",
    "explanations = explainer.explain(test_instances)\n",
    "nn_explanations = nn_explainer.explain(test_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['capital-gain',\n",
       "  'marital-status',\n",
       "  'capital-loss',\n",
       "  'hours-per-week',\n",
       "  'occupation',\n",
       "  'age',\n",
       "  'sex',\n",
       "  'native-country',\n",
       "  'fnlwgt',\n",
       "  'education-num'],\n",
       " [-0.6916381338688814,\n",
       "  0.12350883830059717,\n",
       "  -0.07013317629306902,\n",
       "  0.05951879371883744,\n",
       "  0.05440253382722002,\n",
       "  0.04146769210204393,\n",
       "  0.025463251237053054,\n",
       "  0.014590300825607144,\n",
       "  0.012842448468914323,\n",
       "  0.01055183746359225])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 4\n",
    "# explanations['LimeTabular'].get_explanations(idx)['features'], explanations['LimeTabular'].get_explanations(idx)['scores']\n",
    "# explanations['ShapTabular'].get_explanations(idx)['features'], explanations['ShapTabular'].get_explanations(idx)['scores']\n",
    "explanations['lime'].get_explanations(idx)['features'], explanations['lime'].get_explanations(idx)['scores']\n",
    "# explanations['shap'].get_explanations(idx)['features'], explanations['shap'].get_explanations(idx)['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['capital-gain',\n",
       "  'capital-loss',\n",
       "  'hours-per-week',\n",
       "  'age',\n",
       "  'marital-status',\n",
       "  'native-country',\n",
       "  'sex',\n",
       "  'fnlwgt',\n",
       "  'race',\n",
       "  'occupation'],\n",
       " [0.7146692980670353,\n",
       "  0.09948085944391453,\n",
       "  0.09032331528379359,\n",
       "  -0.059015724128280496,\n",
       "  0.05270170754617013,\n",
       "  -0.029825946975223127,\n",
       "  -0.019937145021878502,\n",
       "  0.019450162959702547,\n",
       "  -0.016697319733481562,\n",
       "  0.01373394956749946])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanations['LimeTabular'].get_explanations(1)['features'], explanations['LimeTabular'].get_explanations(1)['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['instance', 'features', 'values', 'scores', 'target_label'])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanations['LimeTabular'].get_explanations(0).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 0},\n",
       " 'workclass': {'type': 'categorical',\n",
       "  'encoder': OneHotEncoder(),\n",
       "  'cat_dist': array([3.75905983e-02, 2.93190287e-02, 6.42070349e-02, 2.04741821e-04,\n",
       "         6.94197617e-01, 3.47037386e-02, 7.90712911e-02, 4.05593547e-02,\n",
       "         4.29957823e-04, 1.97166373e-02]),\n",
       "  'index': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])},\n",
       " 'fnlwgt': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 11},\n",
       " 'education': {'type': 'categorical',\n",
       "  'encoder': OneHotEncoder(),\n",
       "  'cat_dist': array([0.02843864, 0.03709922, 0.01345154, 0.00505712, 0.01042136,\n",
       "         0.01955284, 0.01547848, 0.03277917, 0.04219729, 0.16430531,\n",
       "         0.01216166, 0.32316449, 0.0543999 , 0.00169936, 0.01707547,\n",
       "         0.22271815]),\n",
       "  'index': array([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27])},\n",
       " 'education-num': {'type': 'numerical',\n",
       "  'encoder': StandardScaler(),\n",
       "  'index': 28},\n",
       " 'marital-status': {'type': 'categorical',\n",
       "  'encoder': OneHotEncoder(),\n",
       "  'cat_dist': array([0.13580525, 0.00075754, 0.45819172, 0.01285779, 0.32998239,\n",
       "         0.0313255 , 0.03107981]),\n",
       "  'index': array([29, 30, 31, 32, 33, 34, 35])},\n",
       " 'occupation': {'type': 'categorical',\n",
       "  'encoder': OneHotEncoder(),\n",
       "  'cat_dist': array([0.03773392, 0.11488064, 0.00030711, 0.1251382 , 0.12460587,\n",
       "         0.03050653, 0.04242251, 0.06187298, 0.1007944 , 0.00495475,\n",
       "         0.12636665, 0.02012612, 0.1126899 , 0.02960567, 0.0482167 ,\n",
       "         0.01977806]),\n",
       "  'index': array([36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51])},\n",
       " 'relationship': {'type': 'categorical',\n",
       "  'encoder': OneHotEncoder(),\n",
       "  'cat_dist': array([0.40366897, 0.25762663, 0.03083412, 0.15521477, 0.10493018,\n",
       "         0.04772532]),\n",
       "  'index': array([52, 53, 54, 55, 56, 57])},\n",
       " 'race': {'type': 'categorical',\n",
       "  'encoder': OneHotEncoder(),\n",
       "  'cat_dist': array([0.00962287, 0.03110028, 0.09592154, 0.00831252, 0.85504279]),\n",
       "  'index': array([58, 59, 60, 61, 62])},\n",
       " 'sex': {'type': 'categorical',\n",
       "  'encoder': OneHotEncoder(),\n",
       "  'cat_dist': array([0.33151796, 0.66848204]),\n",
       "  'index': array([63, 64])},\n",
       " 'capital-gain': {'type': 'numerical',\n",
       "  'encoder': StandardScaler(),\n",
       "  'index': 65},\n",
       " 'capital-loss': {'type': 'numerical',\n",
       "  'encoder': StandardScaler(),\n",
       "  'index': 66},\n",
       " 'hours-per-week': {'type': 'numerical',\n",
       "  'encoder': StandardScaler(),\n",
       "  'index': 67},\n",
       " 'native-country': {'type': 'categorical',\n",
       "  'encoder': OneHotEncoder(),\n",
       "  'cat_dist': array([1.19364481e-02, 5.73277098e-04, 3.72630113e-03, 2.49785021e-03,\n",
       "         1.74030547e-03, 2.82543712e-03, 2.10884075e-03, 9.21338193e-04,\n",
       "         3.17349822e-03, 2.60022112e-03, 7.78018918e-04, 4.21768150e-03,\n",
       "         1.00323492e-03, 1.80172802e-03, 1.53556365e-03, 2.04741821e-05,\n",
       "         4.09483641e-04, 6.14225462e-04, 3.89009459e-04, 3.09160149e-03,\n",
       "         1.20797674e-03, 7.57544736e-04, 2.14978912e-03, 2.17026330e-03,\n",
       "         1.88362475e-03, 4.70906187e-04, 1.94709471e-02, 1.00323492e-03,\n",
       "         4.70906187e-04, 9.41812375e-04, 6.03988371e-03, 1.78125384e-03,\n",
       "         1.37177020e-03, 3.76724950e-03, 4.29957823e-04, 2.35453094e-03,\n",
       "         1.33082183e-03, 6.14225462e-04, 5.52802916e-04, 8.97424348e-01,\n",
       "         1.76077966e-03, 4.70906187e-04, 5.60992588e-03]),\n",
       "  'index': array([ 68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,\n",
       "          81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,\n",
       "          94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106,\n",
       "         107, 108, 109, 110])}}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
       "       'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
       "       'capital-gain', 'capital-loss', 'hours-per-week', 'native-country'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cwl/miniconda3/envs/pnpenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "feature_names = [\n",
    "   \"Age\", \"Workclass\", \"fnlwgt\", \"Education\",\n",
    "   \"Education-Num\", \"Marital Status\", \"Occupation\",\n",
    "   \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\",\n",
    "   \"Capital Loss\", \"Hours per week\", \"Country\", \"label\"\n",
    "]\n",
    "df = pd.DataFrame(\n",
    "  np.genfromtxt('adult.data', delimiter=', ', dtype=str),\n",
    "  columns=feature_names\n",
    ")\n",
    "tabular_data = Tabular(\n",
    "   df,\n",
    "   categorical_columns=[feature_names[i] for i in [1, 3, 5, 6, 7, 8, 9, 13]],\n",
    "   target_column='label'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32561, 15), (32561, 109))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = TabularTransform().fit(tabular_data)\n",
    "class_names = transformer.class_names\n",
    "x = transformer.transform(tabular_data)\n",
    "\n",
    "df.shape, x.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omnixai.preprocessing.tabular import TabularTransform\n",
    "# Data preprocessing\n",
    "transformer = TabularTransform().fit(tabular_data)\n",
    "class_names = transformer.class_names\n",
    "x = transformer.transform(tabular_data)\n",
    "# Split into training and test datasets\n",
    "train, test, train_labels, test_labels = \\\n",
    "    train_test_split(x[:, :-1], x[:, -1], train_size=0.80)\n",
    "# Train an XGBoost model (the last column of `x` is the label column after transformation)\n",
    "model = xgboost.XGBClassifier(n_estimators=300, max_depth=5)\n",
    "model.fit(train, train_labels)\n",
    "# Convert the transformed data back to Tabular instances\n",
    "train_data = transformer.invert(train)\n",
    "test_data = transformer.invert(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.000e+00, 1.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
       "         6.000e+01],\n",
       "        [0.000e+00, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
       "         5.000e+01],\n",
       "        [0.000e+00, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
       "         4.000e+01],\n",
       "        ...,\n",
       "        [1.000e+00, 0.000e+00, 0.000e+00, ..., 7.688e+03, 0.000e+00,\n",
       "         5.000e+01],\n",
       "        [0.000e+00, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
       "         5.000e+01],\n",
       "        [0.000e+00, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
       "         4.000e+01]]),\n",
       " (26048, 108))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Explainer aaa is not found. Please choose explainers from dict_keys(['LimeTabular', 'ShapTabular', 'PartialDependenceTabular', 'ALE', 'SensitivityAnalysisTabular', 'L2XTabular', 'PermutationImportance', 'GlobalShapTabular', 'BiasAnalyzer', 'MACEExplainer', 'GPTExplainer', 'CounterfactualExplainer', 'KNNCounterfactualExplainer', 'IntegratedGradientTabular', 'LinearRegression', 'LogisticRegression', 'TreeRegressor', 'TreeClassifier', 'ShapTreeTabular', 'lime', 'shap', 'pdp', 'partial_dependence', 'ale', 'accumulated_local_effects', 'sa', 'sensitivity', 'l2x', 'L2X', 'permutation', 'shap_global', 'bias', 'mace', 'gpt', 'ce', 'counterfactual', 'ce_knn', 'knn_ce', 'ig', 'integrated_gradient', 'linear_regression', 'logistic_regression', 'tree_regressor', 'tree_classifier', 'shap_tree'])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01momnixai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexplainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtabular\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TabularExplainer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Initialize a TabularExplainer\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m explainer \u001b[38;5;241m=\u001b[39m \u001b[43mTabularExplainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[43mexplainers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshap\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmace\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpdp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43male\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maaa\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# The explainers to apply\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclassification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# The task type\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                                   \u001b[49m\u001b[38;5;66;43;03m# The data for initializing the explainers\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                                       \u001b[49m\u001b[38;5;66;43;03m# The ML model to explain\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m  \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Converts raw features into the model inputs\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m  \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmace\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mignored_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSex\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRace\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRelationship\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCapital Loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m  \u001b[49m\u001b[43m}\u001b[49m\u001b[43m                                                  \u001b[49m\u001b[38;5;66;43;03m# Additional parameters\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pnpenv/lib/python3.10/site-packages/omnixai/explainers/tabular/auto.py:68\u001b[0m, in \u001b[0;36mTabularExplainer.__init__\u001b[0;34m(self, explainers, mode, data, model, preprocess, postprocess, params)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     44\u001b[0m         explainers: Collection,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m         params: Dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     51\u001b[0m ):\n\u001b[1;32m     52\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    :param explainers: The names or alias of the explainers to use.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m    :param mode: The task type, e.g. `classification` or `regression`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m        e.g., `params[\"lime\"] = {\"param_1\": param_1, ...}`.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexplainers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplainers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpostprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpostprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_info \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     79\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: data\u001b[38;5;241m.\u001b[39mcategorical_columns,\n\u001b[1;32m     80\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_column\u001b[39m\u001b[38;5;124m\"\u001b[39m: data\u001b[38;5;241m.\u001b[39mtarget_column,\n\u001b[1;32m     81\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: data\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m     82\u001b[0m         }\n",
      "File \u001b[0;32m~/miniconda3/envs/pnpenv/lib/python3.10/site-packages/omnixai/explainers/base.py:154\u001b[0m, in \u001b[0;36mAutoExplainerBase.__init__\u001b[0;34m(self, explainers, mode, data, model, preprocess, postprocess, params)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m explainers:\n\u001b[1;32m    152\u001b[0m     name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m--> 154\u001b[0m         name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_NAME_TO_CLASS\n\u001b[1;32m    155\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplainer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not found. Please choose explainers from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_NAME_TO_CLASS\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames \u001b[38;5;241m=\u001b[39m explainers\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m=\u001b[39m mode\n",
      "\u001b[0;31mAssertionError\u001b[0m: Explainer aaa is not found. Please choose explainers from dict_keys(['LimeTabular', 'ShapTabular', 'PartialDependenceTabular', 'ALE', 'SensitivityAnalysisTabular', 'L2XTabular', 'PermutationImportance', 'GlobalShapTabular', 'BiasAnalyzer', 'MACEExplainer', 'GPTExplainer', 'CounterfactualExplainer', 'KNNCounterfactualExplainer', 'IntegratedGradientTabular', 'LinearRegression', 'LogisticRegression', 'TreeRegressor', 'TreeClassifier', 'ShapTreeTabular', 'lime', 'shap', 'pdp', 'partial_dependence', 'ale', 'accumulated_local_effects', 'sa', 'sensitivity', 'l2x', 'L2X', 'permutation', 'shap_global', 'bias', 'mace', 'gpt', 'ce', 'counterfactual', 'ce_knn', 'knn_ce', 'ig', 'integrated_gradient', 'linear_regression', 'logistic_regression', 'tree_regressor', 'tree_classifier', 'shap_tree'])"
     ]
    }
   ],
   "source": [
    "# OmniXAI는 LIME, SHAP, TreeSHAP, IG를 갖추고 있다.\n",
    "from omnixai.explainers.tabular import TabularExplainer\n",
    "# Initialize a TabularExplainer\n",
    "explainer = TabularExplainer(\n",
    "  explainers=[\"lime\", \"shap\", \"mace\", \"pdp\", \"ale\", \"aaa\"], # The explainers to apply\n",
    "  mode=\"classification\",                             # The task type\n",
    "  data=train_data,                                   # The data for initializing the explainers\n",
    "  model=model,                                       # The ML model to explain\n",
    "  preprocess=lambda z: transformer.transform(z),     # Converts raw features into the model inputs\n",
    "  params={\n",
    "     \"mace\": {\"ignored_features\": [\"Sex\", \"Race\", \"Relationship\", \"Capital Loss\"]}\n",
    "  }                                                  # Additional parameters\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912a3adb554c416eb8cce4a8f53d0d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('predict',\n",
       "              <omnixai.explanations.base.PredictedResults at 0x7f949ee72a10>),\n",
       "             ('lime',\n",
       "              [{'instance':     Age Workclass    fnlwgt     Education  Education-Num Marital Status  \\\n",
       "              0  20.0   Private  110998.0  Some-college           10.0  Never-married   \n",
       "              \n",
       "                   Occupation Relationship                Race     Sex  Capital Gain  \\\n",
       "              0  Adm-clerical    Own-child  Asian-Pac-Islander  Female           0.0   \n",
       "              \n",
       "                 Capital Loss  Hours per week        Country  \n",
       "              0           0.0            30.0  United-States  , 'features': ['Capital Gain', 'Age', 'Marital Status', 'Hours per week', 'Capital Loss', 'Sex', 'fnlwgt', 'Occupation', 'Relationship', 'Race'], 'values': [0.0, 20.0, 'Never-married', 30.0, 0.0, 'Female', 110998.0, 'Adm-clerical', 'Own-child', 'Asian-Pac-Islander'], 'scores': [0.7098656386832946, 0.0986917834142603, 0.09079057400844752, 0.08627590231720923, 0.08572035980061805, 0.026468281375671122, 0.020150077226355142, 0.016527966979101773, 0.012575063460454554, -0.0117097220354036], 'target_label': 0}, {'instance':     Age Workclass    fnlwgt Education  Education-Num Marital Status  \\\n",
       "              1  18.0         ?  120243.0      11th            7.0  Never-married   \n",
       "              \n",
       "                Occupation Relationship   Race   Sex  Capital Gain  Capital Loss  \\\n",
       "              1          ?    Own-child  White  Male           0.0           0.0   \n",
       "              \n",
       "                 Hours per week        Country  \n",
       "              1            27.0  United-States  , 'features': ['Capital Gain', 'Age', 'Marital Status', 'Hours per week', 'Education-Num', 'Occupation', 'Capital Loss', 'Race', 'Sex', 'Relationship'], 'values': [0.0, 18.0, 'Never-married', 27.0, 7.0, '?', 0.0, 'White', 'Male', 'Own-child'], 'scores': [0.7090524277878573, 0.1003275855623939, 0.08638455437086137, 0.06821804853736921, 0.06510423724484057, 0.046334152039778136, 0.030295790663142252, -0.029545474050139728, -0.024968268738142275, 0.019654032638692743], 'target_label': 0}, {'instance':     Age Workclass    fnlwgt  Education  Education-Num      Marital Status  \\\n",
       "              2  44.0   Private  145160.0  Doctorate           16.0  Married-civ-spouse   \n",
       "              \n",
       "                     Occupation Relationship   Race   Sex  Capital Gain  Capital Loss  \\\n",
       "              2  Prof-specialty      Husband  White  Male           0.0           0.0   \n",
       "              \n",
       "                 Hours per week        Country  \n",
       "              2            40.0  United-States  , 'features': ['Capital Gain', 'Marital Status', 'Education-Num', 'Hours per week', 'Capital Loss', 'Age', 'Occupation', 'Sex', 'Education', 'Race'], 'values': [0.0, 'Married-civ-spouse', 16.0, 40.0, 0.0, 44.0, 'Prof-specialty', 'Male', 'Doctorate', 'White'], 'scores': [-0.710991728171513, 0.12959306198640586, 0.08616408648942103, -0.08201452993464242, -0.07636508995437756, 0.06771270420283163, 0.058932045514993266, 0.03560462266393449, 0.020225307163675862, 0.019273499178806715], 'target_label': 1}, {'instance':     Age  Workclass    fnlwgt Education  Education-Num Marital Status  \\\n",
       "              3  64.0  Local-gov  190660.0   HS-grad            9.0       Divorced   \n",
       "              \n",
       "                   Occupation   Relationship   Race   Sex  Capital Gain  Capital Loss  \\\n",
       "              3  Craft-repair  Not-in-family  White  Male           0.0           0.0   \n",
       "              \n",
       "                 Hours per week        Country  \n",
       "              3            40.0  United-States  , 'features': ['Capital Gain', 'Hours per week', 'Education-Num', 'Marital Status', 'Capital Loss', 'Age', 'Sex', 'Workclass', 'Race', 'fnlwgt'], 'values': [0.0, 40.0, 9.0, 'Divorced', 0.0, 64.0, 'Male', 'Local-gov', 'White', 190660.0], 'scores': [0.7198091371583134, 0.08321620077079614, 0.07533508506889734, 0.06646758355206978, 0.06047291934214548, -0.0533561319531523, -0.02735279124354831, 0.011805904662298323, -0.011532648134842553, -0.009964229903903223], 'target_label': 0}, {'instance':     Age Workclass    fnlwgt Education  Education-Num      Marital Status  \\\n",
       "              4  31.0         ?  233371.0   HS-grad            9.0  Married-civ-spouse   \n",
       "              \n",
       "                Occupation Relationship   Race     Sex  Capital Gain  Capital Loss  \\\n",
       "              4          ?         Wife  Black  Female           0.0           0.0   \n",
       "              \n",
       "                 Hours per week        Country  \n",
       "              4            45.0  United-States  , 'features': ['Capital Gain', 'Marital Status', 'Education-Num', 'Capital Loss', 'Relationship', 'Hours per week', 'Occupation', 'Sex', 'Workclass', 'Race'], 'values': [0.0, 'Married-civ-spouse', 9.0, 0.0, 'Wife', 45.0, '?', 'Female', '?', 'Black'], 'scores': [0.7180871095338015, -0.12065959355737099, 0.07574458916176399, 0.05394884448669542, -0.04908756888961463, -0.048859341765735254, 0.03849209420338807, 0.033499105131228495, 0.010750149697384725, 0.008657066209454762], 'target_label': 0}]),\n",
       "             ('shap',\n",
       "              [{'instance':     Age Workclass    fnlwgt     Education  Education-Num Marital Status  \\\n",
       "              0  20.0   Private  110998.0  Some-college           10.0  Never-married   \n",
       "              \n",
       "                   Occupation Relationship                Race     Sex  Capital Gain  \\\n",
       "              0  Adm-clerical    Own-child  Asian-Pac-Islander  Female           0.0   \n",
       "              \n",
       "                 Capital Loss  Hours per week        Country  \n",
       "              0           0.0            30.0  United-States  , 'features': ['Workclass', 'Education'], 'values': ['Private', 'Some-college'], 'scores': [0.06921174170097814, -0.06917107931417837], 'target_label': 0}, {'instance':     Age Workclass    fnlwgt Education  Education-Num Marital Status  \\\n",
       "              1  18.0         ?  120243.0      11th            7.0  Never-married   \n",
       "              \n",
       "                Occupation Relationship   Race   Sex  Capital Gain  Capital Loss  \\\n",
       "              1          ?    Own-child  White  Male           0.0           0.0   \n",
       "              \n",
       "                 Hours per week        Country  \n",
       "              1            27.0  United-States  , 'features': ['Workclass', 'Education'], 'values': ['?', '11th'], 'scores': [0.0, 0.0], 'target_label': 0}, {'instance':     Age Workclass    fnlwgt  Education  Education-Num      Marital Status  \\\n",
       "              2  44.0   Private  145160.0  Doctorate           16.0  Married-civ-spouse   \n",
       "              \n",
       "                     Occupation Relationship   Race   Sex  Capital Gain  Capital Loss  \\\n",
       "              2  Prof-specialty      Husband  White  Male           0.0           0.0   \n",
       "              \n",
       "                 Hours per week        Country  \n",
       "              2            40.0  United-States  , 'features': ['Workclass', 'Education'], 'values': ['Private', 'Doctorate'], 'scores': [1.2819989119573547, -1.2819151090458614], 'target_label': 1}, {'instance':     Age  Workclass    fnlwgt Education  Education-Num Marital Status  \\\n",
       "              3  64.0  Local-gov  190660.0   HS-grad            9.0       Divorced   \n",
       "              \n",
       "                   Occupation   Relationship   Race   Sex  Capital Gain  Capital Loss  \\\n",
       "              3  Craft-repair  Not-in-family  White  Male           0.0           0.0   \n",
       "              \n",
       "                 Hours per week        Country  \n",
       "              3            40.0  United-States  , 'features': ['Workclass', 'Education'], 'values': ['Local-gov', 'HS-grad'], 'scores': [0.5568852080206841, -0.556796864293976], 'target_label': 0}, {'instance':     Age Workclass    fnlwgt Education  Education-Num      Marital Status  \\\n",
       "              4  31.0         ?  233371.0   HS-grad            9.0  Married-civ-spouse   \n",
       "              \n",
       "                Occupation Relationship   Race     Sex  Capital Gain  Capital Loss  \\\n",
       "              4          ?         Wife  Black  Female           0.0           0.0   \n",
       "              \n",
       "                 Hours per week        Country  \n",
       "              4            45.0  United-States  , 'features': ['Workclass', 'Education'], 'values': ['?', 'HS-grad'], 'scores': [0.4936640042221331, -0.49358347660893637], 'target_label': 0}]),\n",
       "             ('mace',\n",
       "              [{'query':     Age Workclass    fnlwgt     Education  Education-Num Marital Status  \\\n",
       "              0  20.0   Private  110998.0  Some-college           10.0  Never-married   \n",
       "              \n",
       "                   Occupation Relationship                Race     Sex  Capital Gain  \\\n",
       "              0  Adm-clerical    Own-child  Asian-Pac-Islander  Female           0.0   \n",
       "              \n",
       "                 Capital Loss  Hours per week        Country  label  \n",
       "              0           0.0            30.0  United-States      0  , 'counterfactual':         Age Workclass    fnlwgt     Education  Education-Num Marital Status  \\\n",
       "              0  22.34375   Private  110998.0  Some-college           10.0  Never-married   \n",
       "              \n",
       "                   Occupation Relationship                Race     Sex  Capital Gain  \\\n",
       "              0  Adm-clerical    Own-child  Asian-Pac-Islander  Female     12499.875   \n",
       "              \n",
       "                 Capital Loss  Hours per week        Country  label  \n",
       "              0           0.0            30.0  United-States      1  }, {'query':     Age Workclass    fnlwgt Education  Education-Num Marital Status  \\\n",
       "              1  18.0         ?  120243.0      11th            7.0  Never-married   \n",
       "              \n",
       "                Occupation Relationship   Race   Sex  Capital Gain  Capital Loss  \\\n",
       "              1          ?    Own-child  White  Male           0.0           0.0   \n",
       "              \n",
       "                 Hours per week        Country  label  \n",
       "              1            27.0  United-States      0  , 'counterfactual':       Age Workclass    fnlwgt Education  Education-Num Marital Status  \\\n",
       "              1  22.375         ?  120243.0      11th            7.0  Never-married   \n",
       "              \n",
       "                Occupation Relationship   Race   Sex  Capital Gain  Capital Loss  \\\n",
       "              1          ?    Own-child  White  Male       8696.25           0.0   \n",
       "              \n",
       "                 Hours per week        Country  label  \n",
       "              1            27.0  United-States      1  }, {'query':     Age Workclass    fnlwgt  Education  Education-Num      Marital Status  \\\n",
       "              2  44.0   Private  145160.0  Doctorate           16.0  Married-civ-spouse   \n",
       "              \n",
       "                     Occupation Relationship   Race   Sex  Capital Gain  Capital Loss  \\\n",
       "              2  Prof-specialty      Husband  White  Male           0.0           0.0   \n",
       "              \n",
       "                 Hours per week        Country  label  \n",
       "              2            40.0  United-States      1  , 'counterfactual':      Age  Workclass        fnlwgt  Education  Education-Num  \\\n",
       "              1  38.75    Private  145160.00000  Doctorate        14.8750   \n",
       "              0  38.75    Private  144865.59375  Doctorate        14.8750   \n",
       "              1  38.75  State-gov  145160.00000  Doctorate        12.9375   \n",
       "              1  44.00  State-gov  142811.50000  Doctorate        16.0000   \n",
       "              4  44.00    Private  145160.00000  Assoc-voc        16.0000   \n",
       "              \n",
       "                     Marital Status         Occupation Relationship   Race   Sex  \\\n",
       "              1  Married-civ-spouse  Machine-op-inspct      Husband  White  Male   \n",
       "              0  Married-civ-spouse       Craft-repair      Husband  White  Male   \n",
       "              1  Married-civ-spouse     Prof-specialty      Husband  White  Male   \n",
       "              1  Married-civ-spouse     Prof-specialty      Husband  White  Male   \n",
       "              4  Married-civ-spouse       Adm-clerical      Husband  White  Male   \n",
       "              \n",
       "                 Capital Gain  Capital Loss  Hours per week        Country  label  \n",
       "              1        0.0000           0.0            40.0  United-States      0  \n",
       "              0        0.0000           0.0            40.0  United-States      0  \n",
       "              1        0.0000           0.0            40.0  United-States      0  \n",
       "              1      196.0625           0.0            40.0  United-States      0  \n",
       "              4      196.0625           0.0            40.0  United-States      0  }, {'query':     Age  Workclass    fnlwgt Education  Education-Num Marital Status  \\\n",
       "              3  64.0  Local-gov  190660.0   HS-grad            9.0       Divorced   \n",
       "              \n",
       "                   Occupation   Relationship   Race   Sex  Capital Gain  Capital Loss  \\\n",
       "              3  Craft-repair  Not-in-family  White  Male           0.0           0.0   \n",
       "              \n",
       "                 Hours per week        Country  label  \n",
       "              3            40.0  United-States      0  , 'counterfactual':        Age     Workclass      fnlwgt Education  Education-Num  \\\n",
       "              0  64.0000  Self-emp-inc  206992.375   HS-grad         9.0000   \n",
       "              1  64.0000  Self-emp-inc  206992.375   HS-grad         9.0625   \n",
       "              0  64.0000  Self-emp-inc  201548.250   HS-grad         9.0000   \n",
       "              3  64.0000     Local-gov  190660.000   HS-grad         9.0000   \n",
       "              2  62.8125     Local-gov  190660.000   HS-grad         9.0625   \n",
       "              \n",
       "                     Marital Status    Occupation   Relationship   Race   Sex  Capital Gain  \\\n",
       "              0  Married-civ-spouse  Craft-repair  Not-in-family  White  Male           0.0   \n",
       "              1  Married-civ-spouse  Craft-repair  Not-in-family  White  Male           0.0   \n",
       "              0  Married-civ-spouse  Craft-repair  Not-in-family  White  Male           0.0   \n",
       "              3            Divorced  Craft-repair  Not-in-family  White  Male        3103.0   \n",
       "              2            Divorced  Craft-repair  Not-in-family  White  Male        7312.5   \n",
       "              \n",
       "                 Capital Loss  Hours per week        Country  label  \n",
       "              0           0.0         40.0000  United-States      1  \n",
       "              1           0.0         40.0000  United-States      1  \n",
       "              0           0.0         42.1875  United-States      1  \n",
       "              3           0.0         40.0000  United-States      1  \n",
       "              2           0.0         40.3125  United-States      1  }, {'query':     Age Workclass    fnlwgt Education  Education-Num      Marital Status  \\\n",
       "              4  31.0         ?  233371.0   HS-grad            9.0  Married-civ-spouse   \n",
       "              \n",
       "                Occupation Relationship   Race     Sex  Capital Gain  Capital Loss  \\\n",
       "              4          ?         Wife  Black  Female           0.0           0.0   \n",
       "              \n",
       "                 Hours per week        Country  label  \n",
       "              4            45.0  United-States      0  , 'counterfactual':        Age  Workclass        fnlwgt     Education  Education-Num  \\\n",
       "              1  31.0000          ?  233371.00000       HS-grad         9.0000   \n",
       "              0  31.0000          ?  233829.78125       HS-grad         9.0000   \n",
       "              0  31.0000          ?  233371.00000       HS-grad         9.0625   \n",
       "              0  37.1875          ?  233371.00000  Some-college        10.0000   \n",
       "              1  40.0000  Local-gov  233371.00000       HS-grad        10.0000   \n",
       "              \n",
       "                     Marital Status Occupation Relationship   Race     Sex  Capital Gain  \\\n",
       "              1  Married-civ-spouse          ?         Wife  Black  Female           0.0   \n",
       "              0  Married-civ-spouse          ?         Wife  Black  Female           0.0   \n",
       "              0  Married-civ-spouse          ?         Wife  Black  Female           0.0   \n",
       "              0  Married-civ-spouse          ?         Wife  Black  Female           0.0   \n",
       "              1  Married-civ-spouse          ?         Wife  Black  Female           0.0   \n",
       "              \n",
       "                 Capital Loss  Hours per week        Country  label  \n",
       "              1           0.0            54.0  United-States      1  \n",
       "              0           0.0            54.0  United-States      1  \n",
       "              0           0.0            54.0  United-States      1  \n",
       "              0           0.0            45.0  United-States      1  \n",
       "              1           0.0            45.0  United-States      1  }])])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate explanations\n",
    "test_instances = test_data[:5]\n",
    "local_explanations = explainer.explain(X=test_instances)\n",
    "\n",
    "\n",
    "\n",
    "local_explanations\n",
    "# global_explanations = explainer.explain_global(\n",
    "#     params={\"pdp\": {\"features\": [\"Age\", \"Education-Num\", \"Capital Gain\",\n",
    "#                                  \"Capital Loss\", \"Hours per week\", \"Education\",\n",
    "#                                  \"Marital Status\", \"Occupation\"]}}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instance':     Age         Workclass    fnlwgt  Education  Education-Num  \\\n",
       " 1  48.0  Self-emp-not-inc  160724.0  Bachelors           13.0   \n",
       " \n",
       "        Marital Status       Occupation Relationship                Race   Sex  \\\n",
       " 1  Married-civ-spouse  Exec-managerial      Husband  Asian-Pac-Islander  Male   \n",
       " \n",
       "    Capital Gain  Capital Loss  Hours per week Country  \n",
       " 1           0.0           0.0            40.0   Japan  ,\n",
       " 'features': ['Capital Gain',\n",
       "  'Marital Status',\n",
       "  'Hours per week',\n",
       "  'Education-Num',\n",
       "  'Age',\n",
       "  'Occupation',\n",
       "  'Country',\n",
       "  'Capital Loss',\n",
       "  'Sex',\n",
       "  'Race'],\n",
       " 'values': [0.0,\n",
       "  'Married-civ-spouse',\n",
       "  40.0,\n",
       "  13.0,\n",
       "  48.0,\n",
       "  'Exec-managerial',\n",
       "  'Japan',\n",
       "  0.0,\n",
       "  'Male',\n",
       "  'Asian-Pac-Islander'],\n",
       " 'scores': [-0.7174269202573026,\n",
       "  0.12373506933540021,\n",
       "  -0.08009449263879101,\n",
       "  0.07463333708307905,\n",
       "  0.07408962211553939,\n",
       "  0.07013468779567734,\n",
       "  -0.0497145992611175,\n",
       "  -0.042112685273002576,\n",
       "  0.030125166776901604,\n",
       "  0.009174592742435584],\n",
       " 'target_label': 1}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_explanations['lime'].get_explanations(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load(\"data/Wine Quality/X_test.npy\")\n",
    "y_test = np.load(\"data/Wine Quality/y_test.npy\")\n",
    "\n",
    "input_data = np.concatenate((X_test, y_test.reshape(-1, 1)), axis=1)\n",
    "tabular_data = Tabular(\n",
    "   input_data,\n",
    "   categorical_columns=[],\n",
    "   target_column=input_data.shape[1] - 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Explainer lime -- training_data should be an instance of Tabular.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/pnpenv/lib/python3.10/site-packages/omnixai/explainers/base.py:214\u001b[0m, in \u001b[0;36mAutoExplainerBase._build_explainers\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_function\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m _signature:\n\u001b[0;32m--> 214\u001b[0m     explainer \u001b[38;5;241m=\u001b[39m \u001b[43m_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredict_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_param\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m _signature:\n",
      "File \u001b[0;32m~/miniconda3/envs/pnpenv/lib/python3.10/site-packages/omnixai/explainers/tabular/agnostic/lime.py:52\u001b[0m, in \u001b[0;36mLimeTabular.__init__\u001b[0;34m(self, training_data, predict_function, mode, ignored_features, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m:param training_data: The data used to train local explainers in LIME. ``training_data``\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    can be the training dataset for training the machine learning model. If the training\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    `lime_tabular.LimeTabularExplainer`.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignored_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(ignored_features) \u001b[38;5;28;01mif\u001b[39;00m ignored_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/pnpenv/lib/python3.10/site-packages/omnixai/explainers/tabular/base.py:95\u001b[0m, in \u001b[0;36mTabularExplainer.__init__\u001b[0;34m(self, training_data, predict_function, mode, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(training_data, Tabular), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_data should be an instance of Tabular.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_columns \u001b[38;5;241m=\u001b[39m training_data\u001b[38;5;241m.\u001b[39mcategorical_columns\n",
      "\u001b[0;31mAssertionError\u001b[0m: training_data should be an instance of Tabular.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m train_data \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39minvert(torch\u001b[38;5;241m.\u001b[39mtensor(train))\n\u001b[1;32m     12\u001b[0m test_data \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39minvert(torch\u001b[38;5;241m.\u001b[39mtensor(test))\n\u001b[0;32m---> 14\u001b[0m explainer \u001b[38;5;241m=\u001b[39m \u001b[43mTabularExplainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m  \u001b[49m\u001b[43mexplainers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshap\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# The explainers to apply\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclassification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# The task type\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                                       \u001b[49m\u001b[38;5;66;43;03m# The ML model to explain\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m  \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Converts raw features into the model inputs\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pnpenv/lib/python3.10/site-packages/omnixai/explainers/tabular/auto.py:68\u001b[0m, in \u001b[0;36mTabularExplainer.__init__\u001b[0;34m(self, explainers, mode, data, model, preprocess, postprocess, params)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     44\u001b[0m         explainers: Collection,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m         params: Dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     51\u001b[0m ):\n\u001b[1;32m     52\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    :param explainers: The names or alias of the explainers to use.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m    :param mode: The task type, e.g. `classification` or `regression`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m        e.g., `params[\"lime\"] = {\"param_1\": param_1, ...}`.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexplainers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplainers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpostprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpostprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_info \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     79\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: data\u001b[38;5;241m.\u001b[39mcategorical_columns,\n\u001b[1;32m     80\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_column\u001b[39m\u001b[38;5;124m\"\u001b[39m: data\u001b[38;5;241m.\u001b[39mtarget_column,\n\u001b[1;32m     81\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: data\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m     82\u001b[0m         }\n",
      "File \u001b[0;32m~/miniconda3/envs/pnpenv/lib/python3.10/site-packages/omnixai/explainers/base.py:165\u001b[0m, in \u001b[0;36mAutoExplainerBase.__init__\u001b[0;34m(self, explainers, mode, data, model, preprocess, postprocess, params)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams \u001b[38;5;241m=\u001b[39m params\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplainers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_explainers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_info \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniconda3/envs/pnpenv/lib/python3.10/site-packages/omnixai/explainers/base.py:237\u001b[0m, in \u001b[0;36mAutoExplainerBase._build_explainers\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    235\u001b[0m         explainers[name] \u001b[38;5;241m=\u001b[39m explainer\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 237\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplainer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m explainers\n",
      "\u001b[0;31mAssertionError\u001b[0m: Explainer lime -- training_data should be an instance of Tabular."
     ]
    }
   ],
   "source": [
    "nn_model = torch.nn.Linear(X_test.shape[1], 2)\n",
    "\n",
    "transformer = TabularTransform().fit(tabular_data)\n",
    "class_names = transformer.class_names\n",
    "x = transformer.transform(tabular_data)\n",
    "# Split into training and test datasets\n",
    "train, test, train_labels, test_labels = \\\n",
    "    train_test_split(x[:, :-1], x[:, -1], train_size=0.80)\n",
    "# Train an XGBoost model (the last column of `x` is the label column after transformation)\n",
    "# Convert the transformed data back to Tabular instances\n",
    "train_data = transformer.invert(torch.tensor(train))\n",
    "test_data = transformer.invert(torch.tensor(test))\n",
    "\n",
    "explainer = TabularExplainer(\n",
    "  explainers=[\"lime\", \"shap\", \"ig\"], # The explainers to apply\n",
    "  mode=\"classification\",                             # The task type\n",
    "  data=torch.randn(100,2),\n",
    "  model=nn_model,                                       # The ML model to explain\n",
    "  preprocess=lambda z: transformer.transform(z),     # Converts raw features into the model inputs\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenXAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from openxai.dataloader import ReturnLoaders\n",
    "loader_train, loader_test = ReturnLoaders(data_name='german', download=True)\n",
    "# get an input instance from the test dataset\n",
    "inputs, labels = next(iter(loader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openxai import LoadModel\n",
    "model = LoadModel(data_name= 'german', ml_model='ann', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor([d[0] for d in loader_train.dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openxai import Explainer\n",
    "from openxai.experiment_utils import fill_param_dict\n",
    "param_dict = fill_param_dict('lime', {}, X_train)\n",
    "exp_method = Explainer(method='lime', model=model, param_dict=param_dict)\n",
    "explanations= exp_method.get_explanations(inputs, label=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 60])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 60])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openxai import Evaluator\n",
    "from openxai.explainers.perturbation_methods import NormalPerturbation\n",
    "import numpy as np\n",
    "\n",
    "feature_metadata = np.where((inputs.numpy() == 0.0) | (inputs.numpy() == 1.0), 'd', 'c')[0].tolist()\n",
    "\n",
    "metric_evaluator = Evaluator(model, metric='PGI')\n",
    "kwargs = {\n",
    "    \"explanations\": explanations,\n",
    "    \"inputs\": inputs,\n",
    "    \"k\": 0.1,\n",
    "    \"perturb_method\": NormalPerturbation('tabular'),\n",
    "    \"feature_metadata\": feature_metadata\n",
    "}\n",
    "\n",
    "score = metric_evaluator.evaluate(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.61782783, 0.20031753, 0.17158128, 0.32372916, 0.4453343 ,\n",
       "        0.00382134, 0.1159284 , 0.01427623, 0.41874745, 0.15988487,\n",
       "        0.00148061, 0.52968276, 0.19574705, 0.08563896, 0.26961112,\n",
       "        0.19825181, 0.39603913, 0.26493934, 0.10648002, 0.48599267,\n",
       "        0.01620395, 0.16462477, 0.14538735, 0.3685585 , 0.29140532,\n",
       "        0.49489924, 0.01769307, 0.07826665, 0.35052273, 0.4042034 ,\n",
       "        0.1675385 , 0.06326002], dtype=float32),\n",
       " 0.2364961)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\"AbPC\"]\n",
    "FRAMEWORK = [\"PnPXAI\", \"OmniXAI\", \"OpenXAI\"]\n",
    "MODEL = [\"LogisticRegression\", \"XGBoost\", \"TabResNet\", \"TabTransformer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.nn.Linear(10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnpxai.evaluator.metrics.pixel_flipping import AbPC\n",
    "metric_obj = AbPC(model)\n",
    "inputs = {\"input\" : torch.randn(20, 10)}\n",
    "explanations = torch.randn(20, 10)\n",
    "labels = torch.randint(0, 2, (20,))\n",
    "evals = metric_obj.evaluate(inputs, labels, explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0181,  0.0345,  0.0346,  0.0213,  0.0264, -0.1871,  0.0195, -0.1951,\n",
       "         -0.1298,  0.0007, -0.1259,  0.0353, -0.0921,  0.0799,  0.0821, -0.0270,\n",
       "         -0.0424, -0.0208, -0.1354, -0.2065]),\n",
       " torch.Size([20]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evals, evals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoxai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
