{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 선정 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_input_array(input_array, feature_metadata):\n",
    "    inverted_data = {}\n",
    "    \n",
    "    for col, meta in feature_metadata.items():\n",
    "        if meta['type'] == 'categorical':\n",
    "            # One-hot encoded 된 부분 추출\n",
    "            start_idx, end_idx = meta['index'][0], meta['index'][-1] + 1\n",
    "            cat_data = input_array[:, start_idx:end_idx]\n",
    "            # OneHotEncoder로 복원\n",
    "            inverted_col = meta['encoder'].inverse_transform(cat_data)\n",
    "            inverted_data[col] = inverted_col.flatten()\n",
    "        else:\n",
    "            # 수치형 데이터 복원\n",
    "            idx = meta['index']\n",
    "            num_data = input_array[:, idx].reshape(-1, 1)\n",
    "            inverted_col = meta['encoder'].inverse_transform(num_data)\n",
    "            inverted_data[col] = inverted_col.flatten()\n",
    "    \n",
    "    # 복원된 데이터를 DataFrame으로 변환\n",
    "    inverted_df = pd.DataFrame(inverted_data)\n",
    "    \n",
    "    return inverted_df\n",
    "\n",
    "# # input_array에서 원래의 데이터 복원\n",
    "# inverted_df = invert_input_array(input_array, feature_metadata)\n",
    "\n",
    "# # 결과 확인\n",
    "# print(inverted_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_ids = [2, 144, 222, 186, 14]\n",
    "\n",
    "# # 출력 파일 설정\n",
    "# output_file = \"dataset_info.txt\"\n",
    "\n",
    "# with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#     for dataset_id in dataset_ids:\n",
    "#         dataset = fetch_ucirepo(id=dataset_id)\n",
    "#         f.write(f\"Dataset ID: {dataset_id}\\n\")\n",
    "#         f.write(f\"Dataset: {dataset.metadata.name}\\n\")\n",
    "#         f.write(f\"Dataset size (rows, columns): {dataset.data.features.shape}\\n\")\n",
    "#         f.write(dataset.data.features.head(3).to_markdown() + \"\\n\\n\")\n",
    "        \n",
    "#         # 변수 정보 출력\n",
    "#         if 'additional_info' in dataset.metadata and 'variable_info' in dataset.metadata['additional_info']:\n",
    "#             f.write(\"Variable Info:\\n\")\n",
    "#             f.write(dataset.metadata['additional_info']['variable_info'] + \"\\n\\n\")\n",
    "        \n",
    "#         # 타겟 분포 출력\n",
    "#         ratio = dataset.data.targets.value_counts() / len(dataset.data.targets)\n",
    "#         f.write(\"Target Distribution:\\n\")\n",
    "#         f.write(ratio.to_markdown() + \"\\n\\n\")\n",
    "#         f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "\n",
    "# print(f\"Output saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_ids = [2, 144, 222, 186]\n",
    "# for d_id in dataset_ids:\n",
    "#     raw_data = fetch_ucirepo(id=d_id)\n",
    "#     dataset_nm = raw_data.metadata['name']\n",
    "#     raw_data.data.features.to_csv(f\"data/{dataset_nm}/raw_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 0}, 'workclass': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([3.75905983e-02, 2.93190287e-02, 6.42070349e-02, 2.04741821e-04,\n",
      "       6.94197617e-01, 3.47037386e-02, 7.90712911e-02, 4.05593547e-02,\n",
      "       4.29957823e-04, 1.97166373e-02]), 'index': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])}, 'fnlwgt': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 11}, 'education': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([0.02843864, 0.03709922, 0.01345154, 0.00505712, 0.01042136,\n",
      "       0.01955284, 0.01547848, 0.03277917, 0.04219729, 0.16430531,\n",
      "       0.01216166, 0.32316449, 0.0543999 , 0.00169936, 0.01707547,\n",
      "       0.22271815]), 'index': array([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27])}, 'education-num': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 28}, 'marital-status': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([0.13580525, 0.00075754, 0.45819172, 0.01285779, 0.32998239,\n",
      "       0.0313255 , 0.03107981]), 'index': array([29, 30, 31, 32, 33, 34, 35])}, 'occupation': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([0.03773392, 0.11488064, 0.00030711, 0.1251382 , 0.12460587,\n",
      "       0.03050653, 0.04242251, 0.06187298, 0.1007944 , 0.00495475,\n",
      "       0.12636665, 0.02012612, 0.1126899 , 0.02960567, 0.0482167 ,\n",
      "       0.01977806]), 'index': array([36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51])}, 'relationship': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([0.40366897, 0.25762663, 0.03083412, 0.15521477, 0.10493018,\n",
      "       0.04772532]), 'index': array([52, 53, 54, 55, 56, 57])}, 'race': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([0.00962287, 0.03110028, 0.09592154, 0.00831252, 0.85504279]), 'index': array([58, 59, 60, 61, 62])}, 'sex': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([0.33151796, 0.66848204]), 'index': array([63, 64])}, 'capital-gain': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 65}, 'capital-loss': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 66}, 'hours-per-week': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 67}, 'native-country': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([1.19364481e-02, 5.73277098e-04, 3.72630113e-03, 2.49785021e-03,\n",
      "       1.74030547e-03, 2.82543712e-03, 2.10884075e-03, 9.21338193e-04,\n",
      "       3.17349822e-03, 2.60022112e-03, 7.78018918e-04, 4.21768150e-03,\n",
      "       1.00323492e-03, 1.80172802e-03, 1.53556365e-03, 2.04741821e-05,\n",
      "       4.09483641e-04, 6.14225462e-04, 3.89009459e-04, 3.09160149e-03,\n",
      "       1.20797674e-03, 7.57544736e-04, 2.14978912e-03, 2.17026330e-03,\n",
      "       1.88362475e-03, 4.70906187e-04, 1.94709471e-02, 1.00323492e-03,\n",
      "       4.70906187e-04, 9.41812375e-04, 6.03988371e-03, 1.78125384e-03,\n",
      "       1.37177020e-03, 3.76724950e-03, 4.29957823e-04, 2.35453094e-03,\n",
      "       1.33082183e-03, 6.14225462e-04, 5.52802916e-04, 8.97424348e-01,\n",
      "       1.76077966e-03, 4.70906187e-04, 5.60992588e-03]), 'index': array([ 68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,\n",
      "        81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,\n",
      "        94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106,\n",
      "       107, 108, 109, 110])}}\n",
      "income\n",
      "False     0.760718\n",
      "True      0.239282\n",
      "Name: count, dtype: float64\n",
      "Logistic Regression Train Accuracy: 0.8538376884293502\n",
      "Logistic Regression Test Accuracy: 0.8516736615825571\n",
      "MLP Train Accuracy: 0.9518081539682134\n",
      "MLP Test Accuracy: 0.8195311700276384\n"
     ]
    }
   ],
   "source": [
    "dataset_id = 2\n",
    "dataset = fetch_ucirepo(id=dataset_id)\n",
    "\n",
    "feature_metadata = {}\n",
    "\n",
    "input_data = []\n",
    "start_idx = 0\n",
    "for col in dataset.data.features.columns:\n",
    "    feature_metadata[col] = {}\n",
    "    if dataset.data.features[col].dtype == \"object\":\n",
    "        feature_metadata[col]['type'] = \"categorical\"\n",
    "        onehot = OneHotEncoder(handle_unknown='ignore')\n",
    "        feature_val = dataset.data.features[col].fillna(\"missing\")\n",
    "        preprocessed = onehot.fit_transform(feature_val.values.reshape(-1, 1)).toarray()\n",
    "        cat_dist = feature_val.value_counts(dropna=False) / len(dataset.data.features)\n",
    "        cat_dist = cat_dist.loc[onehot.categories_[0]].values\n",
    "        feature_metadata[col]['encoder'] = onehot\n",
    "        feature_metadata[col]['cat_dist'] = cat_dist\n",
    "        feature_metadata[col]['index'] = np.arange(start_idx, start_idx + preprocessed.shape[1])\n",
    "        start_idx += preprocessed.shape[1]\n",
    "    else:\n",
    "        feature_metadata[col]['type'] = \"numerical\"\n",
    "        scaler = StandardScaler()\n",
    "        preprocessed = scaler.fit_transform(dataset.data.features[col].values.reshape(-1, 1))\n",
    "        feature_metadata[col]['encoder'] = scaler\n",
    "        feature_metadata[col]['index'] = start_idx\n",
    "        start_idx += 1\n",
    "\n",
    "    input_data.append(preprocessed)\n",
    "\n",
    "print(feature_metadata)\n",
    "\n",
    "input_array = np.concatenate(input_data, axis=1)\n",
    "\n",
    "print(dataset.data.targets.isin([\">50K\", \">50K.\"]).value_counts() / len(dataset.data.targets))\n",
    "y = dataset.data.targets.isin([\">50K\", \">50K.\"]).values.astype(int)[:,0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_array, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"Logistic Regression Train Accuracy:\", lr.score(X_train, y_train))\n",
    "print(\"Logistic Regression Test Accuracy:\", lr.score(X_test, y_test))\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000)\n",
    "mlp.fit(X_train, y_train)\n",
    "print(\"MLP Train Accuracy:\", mlp.score(X_train, y_train))\n",
    "print(\"MLP Test Accuracy:\", mlp.score(X_test, y_test))\n",
    "\n",
    "path = f\"data/{dataset.metadata['name']}\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "np.save(f\"{path}/X_train.npy\", X_train)\n",
    "np.save(f\"{path}/X_test.npy\", X_test)\n",
    "np.save(f\"{path}/y_train.npy\", y_train)\n",
    "np.save(f\"{path}/y_test.npy\", y_test)\n",
    "\n",
    "with open(f\"{path}/feature_metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump(feature_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Attribute1': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([0.274, 0.269, 0.063, 0.394]), 'index': array([0, 1, 2, 3])}, 'Attribute2': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 4}, 'Attribute3': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([0.04 , 0.049, 0.53 , 0.088, 0.293]), 'index': array([5, 6, 7, 8, 9])}, 'Attribute4': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([0.234, 0.103, 0.012, 0.181, 0.28 , 0.012, 0.022, 0.05 , 0.009,\n",
      "       0.097]), 'index': array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19])}, 'Attribute5': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 20}, 'Attribute6': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([0.603, 0.103, 0.063, 0.048, 0.183]), 'index': array([21, 22, 23, 24, 25])}, 'Attribute7': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([0.062, 0.172, 0.339, 0.174, 0.253]), 'index': array([26, 27, 28, 29, 30])}, 'Attribute8': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 31}, 'Attribute9': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([0.05 , 0.31 , 0.548, 0.092]), 'index': array([32, 33, 34, 35])}, 'Attribute10': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([0.907, 0.041, 0.052]), 'index': array([36, 37, 38])}, 'Attribute11': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 39}, 'Attribute12': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([0.282, 0.232, 0.332, 0.154]), 'index': array([40, 41, 42, 43])}, 'Attribute13': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 44}, 'Attribute14': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([0.139, 0.047, 0.814]), 'index': array([45, 46, 47])}, 'Attribute15': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([0.179, 0.713, 0.108]), 'index': array([48, 49, 50])}, 'Attribute16': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 51}, 'Attribute17': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([0.022, 0.2  , 0.63 , 0.148]), 'index': array([52, 53, 54, 55])}, 'Attribute18': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 56}, 'Attribute19': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([0.596, 0.404]), 'index': array([57, 58])}, 'Attribute20': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([0.963, 0.037]), 'index': array([59, 60])}}\n",
      "class\n",
      "False    0.7\n",
      "True     0.3\n",
      "Name: count, dtype: float64\n",
      "Logistic Regression Train Accuracy: 0.78375\n",
      "Logistic Regression Test Accuracy: 0.78\n",
      "MLP Train Accuracy: 1.0\n",
      "MLP Test Accuracy: 0.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cwl/miniconda3/envs/pnpenv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset_id = 144\n",
    "dataset = fetch_ucirepo(id=dataset_id)\n",
    "\n",
    "# feature_metadata = {}\n",
    "# for col in dataset.data.features.columns:\n",
    "#     feature_metadata[col] = {}\n",
    "#     if dataset.data.features[col].dtype == \"object\":\n",
    "#         feature_metadata[col]['type'] = \"categorical\"\n",
    "#     else:\n",
    "#         feature_metadata[col]['type'] = \"numerical\"\n",
    "        \n",
    "# feature_metadata\n",
    "\n",
    "feature_metadata = {}\n",
    "input_data = []\n",
    "start_idx = 0\n",
    "for col in dataset.data.features.columns:\n",
    "    feature_metadata[col] = {}\n",
    "    if dataset.data.features[col].dtype == \"object\":\n",
    "        feature_metadata[col]['type'] = \"categorical\"\n",
    "        onehot = OneHotEncoder(handle_unknown='ignore')\n",
    "        feature_val = dataset.data.features[col].fillna(\"missing\")\n",
    "        preprocessed = onehot.fit_transform(feature_val.values.reshape(-1, 1)).toarray()\n",
    "        cat_dist = feature_val.value_counts(dropna=False) / len(dataset.data.features)\n",
    "        cat_dist = cat_dist.loc[onehot.categories_[0]].values\n",
    "        feature_metadata[col]['encoder'] = onehot\n",
    "        feature_metadata[col]['cat_dist'] = cat_dist\n",
    "        feature_metadata[col]['index'] = np.arange(start_idx, start_idx + preprocessed.shape[1])\n",
    "        start_idx += preprocessed.shape[1]\n",
    "    else:\n",
    "        feature_metadata[col]['type'] = \"numerical\"\n",
    "        scaler = StandardScaler()\n",
    "        preprocessed = scaler.fit_transform(dataset.data.features[col].values.reshape(-1, 1))\n",
    "        feature_metadata[col]['encoder'] = scaler\n",
    "        feature_metadata[col]['index'] = start_idx\n",
    "        start_idx += 1\n",
    "\n",
    "    input_data.append(preprocessed)\n",
    "\n",
    "print(feature_metadata)\n",
    "\n",
    "input_array = np.concatenate(input_data, axis=1)\n",
    "\n",
    "print(dataset.data.targets.isin([2]).value_counts() / len(dataset.data.targets))\n",
    "y = dataset.data.targets.isin([2]).values.astype(int)[:,0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_array, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"Logistic Regression Train Accuracy:\", lr.score(X_train, y_train))\n",
    "print(\"Logistic Regression Test Accuracy:\", lr.score(X_test, y_test))\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(16, 16), max_iter=400)\n",
    "mlp.fit(X_train, y_train)\n",
    "print(\"MLP Train Accuracy:\", mlp.score(X_train, y_train))\n",
    "print(\"MLP Test Accuracy:\", mlp.score(X_test, y_test))\n",
    "\n",
    "path = f\"data/{dataset.metadata['name']}\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "np.save(f\"{path}/X_train.npy\", X_train)\n",
    "np.save(f\"{path}/X_test.npy\", X_test)\n",
    "np.save(f\"{path}/y_train.npy\", y_train)\n",
    "np.save(f\"{path}/y_test.npy\", y_test)\n",
    "\n",
    "with open(f\"{path}/feature_metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump(feature_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 0}, 'job': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([0.11437482, 0.21525735, 0.03289023, 0.02742695, 0.20919688,\n",
      "       0.00637013, 0.05007631, 0.03492513, 0.09188029, 0.02074716,\n",
      "       0.16803433, 0.02882042]), 'index': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])}, 'marital': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([0.11517109, 0.60193316, 0.28289576]), 'index': array([13, 14, 15])}, 'education': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([0.04107407, 0.15153392, 0.51319369, 0.29419831]), 'index': array([16, 17, 18, 19])}, 'default': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([0.98197341, 0.01802659]), 'index': array([20, 21])}, 'balance': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 22}, 'housing': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([0.44416182, 0.55583818]), 'index': array([23, 24])}, 'loan': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([0.83977351, 0.16022649]), 'index': array([25, 26])}, 'contact': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([0.64774059, 0.28798301, 0.06427639]), 'index': array([27, 28, 29])}, 'day_of_week': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 30}, 'month': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([0.06485147, 0.13817434, 0.00473336, 0.05859194, 0.03103227,\n",
      "       0.15250713, 0.11813497, 0.01055053, 0.30448342, 0.08781049,\n",
      "       0.01632346, 0.01280662]), 'index': array([31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42])}, 'duration': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 43}, 'campaign': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 44}, 'pdays': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 45}, 'previous': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 46}, 'poutcome': {'type': 'categorical', 'encoder': OneHotEncoder(handle_unknown='ignore'), 'cat_dist': array([0.10840282, 0.81747805, 0.04069806, 0.03342107]), 'index': array([47, 48, 49, 50])}}\n",
      "y    \n",
      "False    0.883015\n",
      "True     0.116985\n",
      "Name: count, dtype: float64\n",
      "Logistic Regression Train Accuracy: 0.9020681265206812\n",
      "Logistic Regression Test Accuracy: 0.9014707508570164\n",
      "MLP Train Accuracy: 0.9211181154611812\n",
      "MLP Test Accuracy: 0.9018024991706293\n"
     ]
    }
   ],
   "source": [
    "dataset_id = 222\n",
    "dataset = fetch_ucirepo(id=dataset_id)\n",
    "\n",
    "# feature_metadata = {}\n",
    "# for col in dataset.data.features.columns:\n",
    "#     feature_metadata[col] = {}\n",
    "#     if dataset.data.features[col].dtype == \"object\":\n",
    "#         feature_metadata[col]['type'] = \"categorical\"\n",
    "#     else:\n",
    "#         feature_metadata[col]['type'] = \"numerical\"\n",
    "        \n",
    "# feature_metadata\n",
    "\n",
    "feature_metadata = {}\n",
    "input_data = []\n",
    "start_idx = 0\n",
    "for col in dataset.data.features.columns:\n",
    "    feature_metadata[col] = {}\n",
    "    if dataset.data.features[col].dtype == \"object\":\n",
    "        feature_metadata[col]['type'] = \"categorical\"\n",
    "        onehot = OneHotEncoder(handle_unknown='ignore')\n",
    "        feature_val = dataset.data.features[col].fillna(\"missing\")\n",
    "        preprocessed = onehot.fit_transform(feature_val.values.reshape(-1, 1)).toarray()\n",
    "        cat_dist = feature_val.value_counts(dropna=False) / len(dataset.data.features)\n",
    "        cat_dist = cat_dist.loc[onehot.categories_[0]].values\n",
    "        feature_metadata[col]['encoder'] = onehot\n",
    "        feature_metadata[col]['cat_dist'] = cat_dist\n",
    "        feature_metadata[col]['index'] = np.arange(start_idx, start_idx + preprocessed.shape[1])\n",
    "        start_idx += preprocessed.shape[1]\n",
    "    else:\n",
    "        feature_metadata[col]['type'] = \"numerical\"\n",
    "        scaler = StandardScaler()\n",
    "        preprocessed = scaler.fit_transform(dataset.data.features[col].values.reshape(-1, 1))\n",
    "        feature_metadata[col]['encoder'] = scaler\n",
    "        feature_metadata[col]['index'] = start_idx\n",
    "        start_idx += 1\n",
    "\n",
    "    input_data.append(preprocessed)\n",
    "\n",
    "print(feature_metadata)\n",
    "\n",
    "input_array = np.concatenate(input_data, axis=1)\n",
    "\n",
    "print(dataset.data.targets.isin(['yes']).value_counts() / len(dataset.data.targets))\n",
    "y = dataset.data.targets.isin(['yes']).values.astype(int)[:,0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_array, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"Logistic Regression Train Accuracy:\", lr.score(X_train, y_train))\n",
    "print(\"Logistic Regression Test Accuracy:\", lr.score(X_test, y_test))\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(16, 16), max_iter=400)\n",
    "mlp.fit(X_train, y_train)\n",
    "print(\"MLP Train Accuracy:\", mlp.score(X_train, y_train))\n",
    "print(\"MLP Test Accuracy:\", mlp.score(X_test, y_test))\n",
    "\n",
    "path = f\"data/{dataset.metadata['name']}\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "np.save(f\"{path}/X_train.npy\", X_train)\n",
    "np.save(f\"{path}/X_test.npy\", X_test)\n",
    "np.save(f\"{path}/y_train.npy\", y_train)\n",
    "np.save(f\"{path}/y_test.npy\", y_test)\n",
    "\n",
    "with open(f\"{path}/feature_metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump(feature_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fixed_acidity': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 0}, 'volatile_acidity': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 1}, 'citric_acid': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 2}, 'residual_sugar': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 3}, 'chlorides': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 4}, 'free_sulfur_dioxide': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 5}, 'total_sulfur_dioxide': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 6}, 'density': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 7}, 'pH': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 8}, 'sulphates': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 9}, 'alcohol': {'type': 'numerical', 'encoder': StandardScaler(), 'index': 10}}\n",
      "quality\n",
      "False      0.803448\n",
      "True       0.196552\n",
      "Name: count, dtype: float64\n",
      "Logistic Regression Train Accuracy: 0.8185491629786416\n",
      "Logistic Regression Test Accuracy: 0.8215384615384616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Train Accuracy: 0.8716567250336733\n",
      "MLP Test Accuracy: 0.8276923076923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cwl/miniconda3/envs/pnpenv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset_id = 186\n",
    "dataset = fetch_ucirepo(id=dataset_id)\n",
    "\n",
    "# feature_metadata = {}\n",
    "# for col in dataset.data.features.columns:\n",
    "#     feature_metadata[col] = {}\n",
    "#     if dataset.data.features[col].dtype == \"object\":\n",
    "#         feature_metadata[col]['type'] = \"categorical\"\n",
    "#     else:\n",
    "#         feature_metadata[col]['type'] = \"numerical\"\n",
    "        \n",
    "# feature_metadata\n",
    "\n",
    "feature_metadata = {}\n",
    "input_data = []\n",
    "start_idx = 0\n",
    "for col in dataset.data.features.columns:\n",
    "    feature_metadata[col] = {}\n",
    "    if dataset.data.features[col].dtype == \"object\":\n",
    "        feature_metadata[col]['type'] = \"categorical\"\n",
    "        onehot = OneHotEncoder(handle_unknown='ignore')\n",
    "        feature_val = dataset.data.features[col].fillna(\"missing\")\n",
    "        preprocessed = onehot.fit_transform(feature_val.values.reshape(-1, 1)).toarray()\n",
    "        cat_dist = feature_val.value_counts(dropna=False) / len(dataset.data.features)\n",
    "        cat_dist = cat_dist.loc[onehot.categories_[0]].values\n",
    "        feature_metadata[col]['encoder'] = onehot\n",
    "        feature_metadata[col]['cat_dist'] = cat_dist\n",
    "        feature_metadata[col]['index'] = np.arange(start_idx, start_idx + preprocessed.shape[1])\n",
    "        start_idx += preprocessed.shape[1]\n",
    "    else:\n",
    "        feature_metadata[col]['type'] = \"numerical\"\n",
    "        scaler = StandardScaler()\n",
    "        preprocessed = scaler.fit_transform(dataset.data.features[col].values.reshape(-1, 1))\n",
    "        feature_metadata[col]['encoder'] = scaler\n",
    "        feature_metadata[col]['index'] = start_idx\n",
    "        start_idx += 1\n",
    "\n",
    "    input_data.append(preprocessed)\n",
    "\n",
    "print(feature_metadata)\n",
    "\n",
    "input_array = np.concatenate(input_data, axis=1)\n",
    "\n",
    "print(dataset.data.targets.isin([7,8,9]).value_counts() / len(dataset.data.targets))\n",
    "y = dataset.data.targets.isin([7,8,9]).values.astype(int)[:,0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_array, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"Logistic Regression Train Accuracy:\", lr.score(X_train, y_train))\n",
    "print(\"Logistic Regression Test Accuracy:\", lr.score(X_test, y_test))\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(16, 16), max_iter=400)\n",
    "mlp.fit(X_train, y_train)\n",
    "print(\"MLP Train Accuracy:\", mlp.score(X_train, y_train))\n",
    "print(\"MLP Test Accuracy:\", mlp.score(X_test, y_test))\n",
    "\n",
    "path = f\"data/{dataset.metadata['name']}\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "np.save(f\"{path}/X_train.npy\", X_train)\n",
    "np.save(f\"{path}/X_test.npy\", X_test)\n",
    "np.save(f\"{path}/y_train.npy\", y_train)\n",
    "np.save(f\"{path}/y_test.npy\", y_test)\n",
    "\n",
    "with open(f\"{path}/feature_metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump(feature_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': {'type': 'categorical'},\n",
       " 'menopause': {'type': 'categorical'},\n",
       " 'tumor-size': {'type': 'categorical'},\n",
       " 'inv-nodes': {'type': 'categorical'},\n",
       " 'node-caps': {'type': 'categorical'},\n",
       " 'deg-malig': {'type': 'numerical'},\n",
       " 'breast': {'type': 'categorical'},\n",
       " 'breast-quad': {'type': 'categorical'},\n",
       " 'irradiat': {'type': 'categorical'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Breast Cancer는 데이터 크기는 작은데 반해 numerical data가 categorical로 들어가 있어서 전처리에 대한 복잡도가 높음.\n",
    "# 이미 데이터셋 크기가 큰 충분한 데이터가 있기 때문에 추가적인 데이터셋의 필요성은 떨어짐.\n",
    "dataset_id = 14\n",
    "dataset = fetch_ucirepo(id=dataset_id)\n",
    "\n",
    "feature_metadata = {}\n",
    "for col in dataset.data.features.columns:\n",
    "    feature_metadata[col] = {}\n",
    "    if dataset.data.features[col].dtype == \"object\":\n",
    "        feature_metadata[col]['type'] = \"categorical\"\n",
    "    else:\n",
    "        feature_metadata[col]['type'] = \"numerical\"\n",
    "        \n",
    "feature_metadata\n",
    "\n",
    "# feature_metadata = {}\n",
    "# input_data = []\n",
    "# start_idx = 0\n",
    "# for col in dataset.data.features.columns:\n",
    "#     feature_metadata[col] = {}\n",
    "#     if dataset.data.features[col].dtype == \"object\":\n",
    "#         feature_metadata[col]['type'] = \"categorical\"\n",
    "#         onehot = OneHotEncoder()\n",
    "#         preprocessed = onehot.fit_transform(dataset.data.features[col].values.reshape(-1, 1)).toarray()\n",
    "#         cat_dist = dataset.data.features[col].value_counts(dropna=False) / len(dataset.data.features)\n",
    "#         cat_dist = cat_dist.loc[onehot.categories_[0]].values\n",
    "#         feature_metadata[col]['encoder'] = onehot\n",
    "#         feature_metadata[col]['cat_dist'] = cat_dist\n",
    "#         feature_metadata[col]['index'] = np.arange(start_idx, start_idx + preprocessed.shape[1])\n",
    "#         start_idx += preprocessed.shape[1]\n",
    "#     else:\n",
    "#         feature_metadata[col]['type'] = \"numerical\"\n",
    "#         scaler = StandardScaler()\n",
    "#         preprocessed = scaler.fit_transform(dataset.data.features[col].values.reshape(-1, 1))\n",
    "#         feature_metadata[col]['encoder'] = scaler\n",
    "#         feature_metadata[col]['index'] = start_idx\n",
    "#         start_idx += 1\n",
    "\n",
    "#     input_data.append(preprocessed)\n",
    "\n",
    "# print(feature_metadata)\n",
    "\n",
    "# input_array = np.concatenate(input_data, axis=1)\n",
    "\n",
    "# print(dataset.data.targets.isin([7,8,9]).value_counts() / len(dataset.data.targets))\n",
    "# y = dataset.data.targets.isin([7,8,9]).values.astype(int)[:,0]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(input_array, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# lr = LogisticRegression(max_iter=1000)\n",
    "# lr.fit(X_train, y_train)\n",
    "# print(\"Logistic Regression Train Accuracy:\", lr.score(X_train, y_train))\n",
    "# print(\"Logistic Regression Test Accuracy:\", lr.score(X_test, y_test))\n",
    "\n",
    "# mlp = MLPClassifier(hidden_layer_sizes=(16, 16), max_iter=400)\n",
    "# mlp.fit(X_train, y_train)\n",
    "# print(\"MLP Train Accuracy:\", mlp.score(X_train, y_train))\n",
    "# print(\"MLP Test Accuracy:\", mlp.score(X_test, y_test))\n",
    "\n",
    "# path = f\"data/{dataset.metadata['name']}\"\n",
    "# if not os.path.exists(path):\n",
    "#     os.makedirs(path)\n",
    "\n",
    "# np.save(f\"{path}/X_train.npy\", X_train)\n",
    "# np.save(f\"{path}/X_test.npy\", X_test)\n",
    "# np.save(f\"{path}/y_train.npy\", y_train)\n",
    "# np.save(f\"{path}/y_test.npy\", y_test)\n",
    "\n",
    "# with open(f\"{path}/feature_metadata.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(feature_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pnpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
