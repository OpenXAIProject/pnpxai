{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96740fb8-2891-43d7-a66f-204019ec8450",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "Please make sure\n",
    "- your virtual environment is activated\n",
    "\n",
    "```\n",
    "source .venv/bin/activate\n",
    "pip install -r requirements.txt\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30df927-ce47-4694-96af-d1a88a71fa96",
   "metadata": {},
   "source": [
    "I suppose that a user  already has the serialized model and dataset just for showing brief of our server's workflow. The following creates them on your current directory.\n",
    "\n",
    "p.s. we should have our own serialization process as Enver said two weeks ago."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60be2d0-8aca-4fd3-b45a-d22d3872100b",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3b592f-ecd0-4189-ba86-bab6315b011a",
   "metadata": {},
   "source": [
    "resnet50 but any torchvision image classifier possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb7092fe-3b25-47e8-b15e-6e8b278d91f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "model_name = 'resnet50'\n",
    "weights = torchvision.models.get_model_weights(model_name).DEFAULT\n",
    "model = torchvision.models.get_model(model_name, weights=weights).eval()\n",
    "torch.save(model, f'{model_name}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e88091-e98a-4f98-8f71-c59e131b1d1a",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b33187-6d73-4874-9fab-8c0901ba777b",
   "metadata": {},
   "source": [
    "1000 images will be downloaded on your current directory.\n",
    "\n",
    "Note that preprocess for a task is conventionally defined at torch.utils.data.Dataset level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab423928-b427-4867-b2da-7f4e0b17a4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'imagenet-sample-images'...\n"
     ]
    }
   ],
   "source": [
    "from pnpxai.samples.datasets import ImageNetSample\n",
    "\n",
    "dataset = ImageNetSample(transform=weights.transforms())\n",
    "torch.save(dataset, \"imagenet_sample.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afe8a621-dfac-4342-a8c3-ca5dc1d14b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gkim/Projects/pnpxai-demo/.venv/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.5870, -1.3987, -1.1932,  ..., -0.3712, -0.4054, -0.4226],\n",
       "          [-1.3815, -1.2617, -1.1247,  ..., -0.3712, -0.3883, -0.4397],\n",
       "          [-1.1760, -1.1589, -0.9705,  ..., -0.3541, -0.3883, -0.4226],\n",
       "          ...,\n",
       "          [ 0.1083,  0.1426,  0.1426,  ...,  1.1015,  0.9817,  1.1358],\n",
       "          [ 0.1768,  0.2282,  0.2453,  ...,  1.2385,  0.9988,  1.0502],\n",
       "          [ 0.1939,  0.1597,  0.1597,  ...,  1.2728,  1.1015,  0.9988]],\n",
       " \n",
       "         [[-0.8978, -0.8277, -0.7227,  ..., -0.6352, -0.6702, -0.7052],\n",
       "          [-0.8452, -0.7927, -0.6702,  ..., -0.6702, -0.6877, -0.7402],\n",
       "          [-0.7052, -0.6877, -0.5476,  ..., -0.6352, -0.6702, -0.7052],\n",
       "          ...,\n",
       "          [ 0.1352,  0.1702,  0.1702,  ...,  0.3978,  0.2752,  0.4328],\n",
       "          [ 0.1702,  0.2052,  0.1877,  ...,  0.5728,  0.3102,  0.2927],\n",
       "          [ 0.2052,  0.1702,  0.1702,  ...,  0.6429,  0.4503,  0.2577]],\n",
       " \n",
       "         [[ 0.3568,  0.3742,  0.4439,  ..., -0.9678, -0.9853, -1.0027],\n",
       "          [ 0.3568,  0.4091,  0.4614,  ..., -0.9156, -0.9504, -0.9853],\n",
       "          [ 0.4265,  0.3916,  0.4788,  ..., -0.9330, -0.9853, -1.0201],\n",
       "          ...,\n",
       "          [ 0.1476,  0.1476,  0.0953,  ..., -0.6715, -0.9330, -0.7587],\n",
       "          [ 0.1825,  0.1999,  0.1302,  ..., -0.3927, -0.8110, -0.8284],\n",
       "          [ 0.1476,  0.0779,  0.0256,  ..., -0.2707, -0.6193, -0.9330]]]),\n",
       " 283)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
