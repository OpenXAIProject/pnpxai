# Evaluator <small>[[source](api/evaluator/metrics.md)]</small>

The Evaluator module provides several metrics to evaluate the result explainations generated by `explainers`. Based on Co-12 properties [1], correctness, continuity, compactness, and completeness are chosen. Currently one metrics for each properties are implemented except completeness.

## Properties of evaluation
| Property [1] | Explanation | Corresponding Metrics | Reference |
| --- | --- | --- | --- |
| Correctness | Evaluates the truth/reliability of the explanation of the prediction model (AI model). In other words, it indicates how truthful the explanation is compared to the behavior of the black box model. | [**MuFidelity**](api/evaluator/metrics.md/#pnpxai.evaluator.metrics.mu_fidelity.MuFidelity) | [2] |
| Continuity | Continuity assesses how continuous (i.e. smooth) the description is. High-continuity explanation functions ensure that small changes in the input do not lead to large changes in the explanation. | [**Sensitivity**](api/evaluator/metrics.md/#pnpxai.evaluator.metrics.sensitivity.Sensitivity) | [3] |
| Compactness | Evaluates the size/amount of explanation. Ensure that you do not present complex and redundant explanations that are difficult to understand. | [**Complexity**](api/evaluator/metrics.md/#pnpxai.evaluator.metrics.complexity.Complexity) | [2] |
| Completeness | Evaluates the degree to which a prediction model (AI model) is explained. Providing “the whole truth” of a black box model has a high degree of completeness, but a good description must balance compactness and correctness. | [**Area between Perturbation Curves**](api/evaluator/metrics.md#pnpxai.evaluator.metrics.pixel_flipping.AbPC) | - |


## Usage

```python
import torch
from torch.utils.data import DataLoader

from pnpxai.explainers import IntegratedGradients
from pnpxai.evaluator.metrics import Complexity

from helpers import get_imagenet_dataset, get_torchvision_model

# load model, dataset, explainer, and metric
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model, transform = get_torchvision_model('resnet18')

dataset = get_imagenet_dataset(transform, indices=range(1000))
loader = DataLoader(dataset, batch_size=4, shuffle=False)

explainer = IntegratedGradients(model)
metric = Complexity(model, explainer)

inputs, targets = next(iter(loader))
inputs, targets = inputs.to(device), targets.to(device)

# make explanation
attrs = explainer.attribute(inputs, targets)

# test evaluator
evaluations = metric(inputs, targets, attrs)
print(evaluations)
```

## Reference

[1] M. Nauta, J. Trienes, S. Pathak, E. Nguyen, M. Peters, Y. Schmitt, J. Schlötterer, M. V. Keulen, C. Seifert. From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI. ACM Comput. Surv. 55(13s): 295:1-295:42 (2023).

[2] U. Bhatt, A. Weller, and J. M. F. Moura. Evaluating and aggregating feature-based model explanations. In Proceedings of the IJCAI (2020).

[3] C.-K. Yeh, C.-Y. Hsieh, A.S. Suggala, D.I. Inouye, and P. Ravikumar. On the (in)fidelity and sensitivity of explanations. In Proceedings of the NeurIPS (2019).