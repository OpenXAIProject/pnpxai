
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://openxaiproject.github.io/pnpxai/api/explainer/attn_rollout/">
      
      
        <link rel="prev" href="../lime/">
      
      
        <link rel="next" href="../../../detector/">
      
      
      <link rel="icon" href="../../../assets/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.33">
    
    
      
        <title>AttentionRollout - PnP XAI Docs</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.3cba04c6.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#pnpxai.explainers.attention_rollout" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="PnP XAI Docs" class="md-header__button md-logo" aria-label="PnP XAI Docs" data-md-component="logo">
      
  <img src="../../../assets/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            PnP XAI Docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              AttentionRollout
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="PnP XAI Docs" class="md-nav__button md-logo" aria-label="PnP XAI Docs" data-md-component="logo">
      
  <img src="../../../assets/logo.png" alt="logo">

    </a>
    PnP XAI Docs
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    API
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            API
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" >
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Core
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            Core
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../core/experiment/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Experiment
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../core/auto_explanation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Auto Explanation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../core/modality/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modality
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../core/recommender/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Recommender
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../core/detector/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Detector
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Evaluator
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            Evaluator
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../evaluator/metrics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Metrics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../evaluator/optimizer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Optimizer
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" checked>
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Explainer
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            Explainer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../grad_cam/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GradCam
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../guided_grad_cam/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GuidedGradCam
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gradient/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Gradient
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../grad_x_input/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GradientXInput
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../smooth_grad/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SmoothGrad
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../var_grad/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    VarGrad
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ig/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    IntegratedGradients
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lrp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LRP
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../rap/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    RAP
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../kernel_shap/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    KernelShap
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lime/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Lime
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    AttentionRollout
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    AttentionRollout
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#pnpxai.explainers.attention_rollout" class="md-nav__link">
    <span class="md-ellipsis">
      attention_rollout
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pnpxai.explainers.attention_rollout.AttentionRollout" class="md-nav__link">
    <span class="md-ellipsis">
      AttentionRollout
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pnpxai.explainers.attention_rollout.AttentionRolloutBase" class="md-nav__link">
    <span class="md-ellipsis">
      AttentionRolloutBase
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AttentionRolloutBase">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pnpxai.explainers.attention_rollout.AttentionRolloutBase.attribute" class="md-nav__link">
    <span class="md-ellipsis">
      attribute
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pnpxai.explainers.attention_rollout.AttentionRolloutBase.get_tunables" class="md-nav__link">
    <span class="md-ellipsis">
      get_tunables
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pnpxai.explainers.attention_rollout.TransformerAttribution" class="md-nav__link">
    <span class="md-ellipsis">
      TransformerAttribution
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../detector/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Detector
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../explainer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Explainer
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../recommender/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Recommender
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../evaluator/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Evaluator
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../optimizer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Optimizer
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#pnpxai.explainers.attention_rollout" class="md-nav__link">
    <span class="md-ellipsis">
      attention_rollout
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pnpxai.explainers.attention_rollout.AttentionRollout" class="md-nav__link">
    <span class="md-ellipsis">
      AttentionRollout
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pnpxai.explainers.attention_rollout.AttentionRolloutBase" class="md-nav__link">
    <span class="md-ellipsis">
      AttentionRolloutBase
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AttentionRolloutBase">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pnpxai.explainers.attention_rollout.AttentionRolloutBase.attribute" class="md-nav__link">
    <span class="md-ellipsis">
      attribute
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pnpxai.explainers.attention_rollout.AttentionRolloutBase.get_tunables" class="md-nav__link">
    <span class="md-ellipsis">
      get_tunables
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pnpxai.explainers.attention_rollout.TransformerAttribution" class="md-nav__link">
    <span class="md-ellipsis">
      TransformerAttribution
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  <h1>AttentionRollout</h1>

<div class="doc doc-object doc-module">



<a id="pnpxai.explainers.attention_rollout"></a>
    <div class="doc doc-contents first">



  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h2 id="pnpxai.explainers.attention_rollout.AttentionRollout" class="doc doc-heading">
            <code>AttentionRollout</code>


</h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="pnpxai.explainers.attention_rollout.AttentionRolloutBase" href="#pnpxai.explainers.attention_rollout.AttentionRolloutBase">AttentionRolloutBase</a></code></p>


      <p>Implementation of <code>AttentionRollout</code> explainer.</p>
<p>Supported Modules: <code>Attention</code></p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>model</code></td>
            <td>
                  <code><span title="torch.nn.modules.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The PyTorch model for which attribution is to be computed.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>interpolate_mode</code></td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The interpolation mode used by the explainer. Available methods are: "bilinear" and "bicubic"</p>
              </div>
            </td>
            <td>
                  <code>&#39;bilinear&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>head_fusion_method</code></td>
            <td>
                  <code><span title="typing.Literal">Literal</span>[&#39;min&#39;, &#39;max&#39;, &#39;mean&#39;]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>(Optional[str]): Method to apply to head fusion. Available methods are: <code>"min"</code>, <code>"max"</code>, <code>"mean"</code></p>
              </div>
            </td>
            <td>
                  <code>&#39;min&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>discard_ratio</code></td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>(Optional[float]): Describes ration of attention values to discard.</p>
              </div>
            </td>
            <td>
                  <code>0.9</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>forward_arg_extractor</code></td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Callable">Callable</span>[[<span title="typing.Tuple">Tuple</span>[<span title="torch.Tensor">Tensor</span>]], <span title="typing.Union">Union</span>[<span title="torch.Tensor">Tensor</span>, <span title="typing.Tuple">Tuple</span>[<span title="torch.Tensor">Tensor</span>]]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Optional function to extract forward arguments from inputs.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>additional_forward_arg_extractor</code></td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Callable">Callable</span>[[<span title="typing.Tuple">Tuple</span>[<span title="torch.Tensor">Tensor</span>]], <span title="typing.Union">Union</span>[<span title="torch.Tensor">Tensor</span>, <span title="typing.Tuple">Tuple</span>[<span title="torch.Tensor">Tensor</span>]]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Optional function to extract additional forward arguments.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>n_classes</code></td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>(Optional[int]): Number of classes</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>**kwargs</code></td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Keyword arguments that are forwarded to the base implementation of the Explainer</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


<details class="reference" open>
  <summary>Reference</summary>
  <p>Samira Abnar, Willem Zuidema. Quantifying Attention Flow in Transformers.</p>
</details>
              <details class="quote">
                <summary>Source code in <code>pnpxai/explainers/attention_rollout.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">AttentionRollout</span><span class="p">(</span><span class="n">AttentionRolloutBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implementation of `AttentionRollout` explainer.</span>

<span class="sd">    Supported Modules: `Attention`</span>

<span class="sd">    Parameters:</span>
<span class="sd">        model (Module): The PyTorch model for which attribution is to be computed.</span>
<span class="sd">        interpolate_mode (Optional[str]): The interpolation mode used by the explainer. Available methods are: &quot;bilinear&quot; and &quot;bicubic&quot;</span>
<span class="sd">        head_fusion_method: (Optional[str]): Method to apply to head fusion. Available methods are: `&quot;min&quot;`, `&quot;max&quot;`, `&quot;mean&quot;`</span>
<span class="sd">        discard_ratio: (Optional[float]): Describes ration of attention values to discard.</span>
<span class="sd">        forward_arg_extractor (Optional[Callable[[Tuple[Tensor]], Union[Tensor, Tuple[Tensor]]]]): Optional function to extract forward arguments from inputs.</span>
<span class="sd">        additional_forward_arg_extractor (Optional[Callable[[Tuple[Tensor]], Union[Tensor, Tuple[Tensor]]]]): Optional function to extract additional forward arguments.</span>
<span class="sd">        n_classes: (Optional[int]): Number of classes</span>
<span class="sd">        **kwargs: Keyword arguments that are forwarded to the base implementation of the Explainer</span>

<span class="sd">    Reference:</span>
<span class="sd">        Samira Abnar, Willem Zuidema. Quantifying Attention Flow in Transformers.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span>
        <span class="n">interpolate_mode</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s1">&#39;bilinear&#39;</span><span class="p">]</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span>
        <span class="n">head_fusion_method</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="s1">&#39;max&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">]</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span>
        <span class="n">discard_ratio</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
        <span class="n">forward_arg_extractor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ForwardArgumentExtractor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">additional_forward_arg_extractor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ForwardArgumentExtractor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">n_classes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span>
            <span class="n">interpolate_mode</span><span class="p">,</span>
            <span class="n">head_fusion_method</span><span class="p">,</span>
            <span class="n">discard_ratio</span><span class="p">,</span>
            <span class="n">forward_arg_extractor</span><span class="p">,</span>
            <span class="n">additional_forward_arg_extractor</span><span class="p">,</span>
            <span class="n">n_classes</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">collect_attention_map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
        <span class="c1"># get all attn maps</span>
        <span class="k">with</span> <span class="n">SavingAttentionAttributor</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">)</span> <span class="k">as</span> <span class="n">attributor</span><span class="p">:</span>
            <span class="n">weights_all</span> <span class="o">=</span> <span class="n">attributor</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">weights_all</span><span class="p">,)</span>

    <span class="k">def</span> <span class="nf">rollout</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights_all</span><span class="p">):</span>
        <span class="n">sz</span> <span class="o">=</span> <span class="n">weights_all</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span>
            <span class="n">attn_weights</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">sz</span>
            <span class="k">for</span> <span class="n">attn_weights</span> <span class="ow">in</span> <span class="n">weights_all</span>
        <span class="p">)</span>
        <span class="n">bsz</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span> <span class="o">=</span> <span class="n">sz</span>
        <span class="n">rollout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">tgt_len</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">attn_weights</span> <span class="ow">in</span> <span class="n">weights_all</span><span class="p">:</span>
            <span class="n">attn_map</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_fusion_function</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">)</span>
            <span class="n">attn_map</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_discard</span><span class="p">(</span><span class="n">attn_map</span><span class="p">)</span>
            <span class="n">identity</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">tgt_len</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">attn_map</span> <span class="o">=</span> <span class="mf">.5</span> <span class="o">*</span> <span class="n">attn_map</span> <span class="o">+</span> <span class="mf">.5</span> <span class="o">*</span> <span class="n">identity</span>
            <span class="n">attn_map</span> <span class="o">/=</span> <span class="n">attn_map</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">rollout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">rollout</span><span class="p">,</span> <span class="n">attn_map</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">rollout</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pnpxai.explainers.attention_rollout.AttentionRolloutBase" class="doc doc-heading">
            <code>AttentionRolloutBase</code>


</h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pnpxai.explainers.zennit.base.ZennitExplainer">ZennitExplainer</span></code></p>


      <p>Base class for <code>AttentionRollout</code> and <code>TransformerAttribution</code> explainers.</p>
<p>Supported Modules: <code>Attention</code></p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>model</code></td>
            <td>
                  <code><span title="torch.nn.modules.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The PyTorch model for which attribution is to be computed.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>interpolate_mode</code></td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The interpolation mode used by the explainer. Available methods are: "bilinear" and "bicubic"</p>
              </div>
            </td>
            <td>
                  <code>&#39;bilinear&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>head_fusion_method</code></td>
            <td>
                  <code><span title="typing.Literal">Literal</span>[&#39;min&#39;, &#39;max&#39;, &#39;mean&#39;]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>(Optional[str]): Method to apply to head fusion. Available methods are: <code>"min"</code>, <code>"max"</code>, <code>"mean"</code></p>
              </div>
            </td>
            <td>
                  <code>&#39;min&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>discard_ratio</code></td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>(Optional[float]): Describes ration of attention values to discard.</p>
              </div>
            </td>
            <td>
                  <code>0.9</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>forward_arg_extractor</code></td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Callable">Callable</span>[[<span title="typing.Tuple">Tuple</span>[<span title="torch.Tensor">Tensor</span>]], <span title="typing.Union">Union</span>[<span title="torch.Tensor">Tensor</span>, <span title="typing.Tuple">Tuple</span>[<span title="torch.Tensor">Tensor</span>]]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Optional function to extract forward arguments from inputs.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>additional_forward_arg_extractor</code></td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Callable">Callable</span>[[<span title="typing.Tuple">Tuple</span>[<span title="torch.Tensor">Tensor</span>]], <span title="typing.Union">Union</span>[<span title="torch.Tensor">Tensor</span>, <span title="typing.Tuple">Tuple</span>[<span title="torch.Tensor">Tensor</span>]]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Optional function to extract additional forward arguments.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>n_classes</code></td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>(Optional[int]): Number of classes</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>forward_arg_extractor</code></td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="pnpxai.explainers.types.ForwardArgumentExtractor">ForwardArgumentExtractor</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A function that extracts forward arguments from the input batch(s) where the attribution scores are assigned.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>additional_forward_arg_extractor</code></td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="pnpxai.explainers.types.ForwardArgumentExtractor">ForwardArgumentExtractor</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A secondary function that extract additional forward arguments from the input batch(s).</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>**kwargs</code></td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Keyword arguments that are forwarded to the base implementation of the Explainer</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


<details class="reference" open>
  <summary>Reference</summary>
  <p>Samira Abnar, Willem Zuidema. Quantifying Attention Flow in Transformers.</p>
</details>
              <details class="quote">
                <summary>Source code in <code>pnpxai/explainers/attention_rollout.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">AttentionRolloutBase</span><span class="p">(</span><span class="n">ZennitExplainer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for `AttentionRollout` and `TransformerAttribution` explainers.</span>

<span class="sd">    Supported Modules: `Attention`</span>

<span class="sd">    Parameters:</span>
<span class="sd">        model (Module): The PyTorch model for which attribution is to be computed.</span>
<span class="sd">        interpolate_mode (Optional[str]): The interpolation mode used by the explainer. Available methods are: &quot;bilinear&quot; and &quot;bicubic&quot;</span>
<span class="sd">        head_fusion_method: (Optional[str]): Method to apply to head fusion. Available methods are: `&quot;min&quot;`, `&quot;max&quot;`, `&quot;mean&quot;`</span>
<span class="sd">        discard_ratio: (Optional[float]): Describes ration of attention values to discard.</span>
<span class="sd">        forward_arg_extractor (Optional[Callable[[Tuple[Tensor]], Union[Tensor, Tuple[Tensor]]]]): Optional function to extract forward arguments from inputs.</span>
<span class="sd">        additional_forward_arg_extractor (Optional[Callable[[Tuple[Tensor]], Union[Tensor, Tuple[Tensor]]]]): Optional function to extract additional forward arguments.</span>
<span class="sd">        n_classes: (Optional[int]): Number of classes</span>
<span class="sd">        forward_arg_extractor: A function that extracts forward arguments from the input batch(s) where the attribution scores are assigned.</span>
<span class="sd">        additional_forward_arg_extractor: A secondary function that extract additional forward arguments from the input batch(s).</span>
<span class="sd">        **kwargs: Keyword arguments that are forwarded to the base implementation of the Explainer</span>

<span class="sd">    Reference:</span>
<span class="sd">        Samira Abnar, Willem Zuidema. Quantifying Attention Flow in Transformers.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">SUPPORTED_MODULES</span> <span class="o">=</span> <span class="p">[</span><span class="n">Attention</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span>
        <span class="n">interpolate_mode</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s1">&#39;bilinear&#39;</span><span class="p">]</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span>
        <span class="n">head_fusion_method</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="s1">&#39;max&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">]</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span>
        <span class="n">discard_ratio</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
        <span class="n">forward_arg_extractor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ForwardArgumentExtractor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">additional_forward_arg_extractor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ForwardArgumentExtractor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">n_classes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span>
            <span class="n">forward_arg_extractor</span><span class="p">,</span>
            <span class="n">additional_forward_arg_extractor</span><span class="p">,</span>
            <span class="n">n_classes</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">interpolate_mode</span> <span class="o">=</span> <span class="n">interpolate_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_fusion_method</span> <span class="o">=</span> <span class="n">head_fusion_method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">discard_ratio</span> <span class="o">=</span> <span class="n">discard_ratio</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">head_fusion_function</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_get_rollout_head_fusion_function</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_fusion_method</span><span class="p">)</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">collect_attention_map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">rollout</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">_discard</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fused_attn_map</span><span class="p">):</span>
        <span class="n">org_size</span> <span class="o">=</span> <span class="n">fused_attn_map</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="c1"># keep size to recover it after discard</span>
        <span class="n">flattened</span> <span class="o">=</span> <span class="n">fused_attn_map</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">bsz</span><span class="p">,</span> <span class="n">n_tokens</span> <span class="o">=</span> <span class="n">flattened</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="n">attn_cls</span> <span class="o">=</span> <span class="n">flattened</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="c1"># keep attn scores of cls token to recover them after discard</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">flattened</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span>
            <span class="n">k</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">n_tokens</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">discard_ratio</span><span class="p">),</span>
            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">largest</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">flattened</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">bsz</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">indices</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.</span> <span class="c1"># discard</span>
        <span class="n">flattened</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">attn_cls</span> <span class="c1"># recover attn scores of cls token</span>
        <span class="n">discarded</span> <span class="o">=</span> <span class="n">flattened</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">org_size</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">discarded</span>

    <span class="k">def</span> <span class="nf">attribute</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="n">targets</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes attributions for the given inputs and targets.</span>

<span class="sd">        Args:</span>
<span class="sd">            inputs (torch.Tensor): The input data.</span>
<span class="sd">            targets (torch.Tensor): The target labels for the inputs.</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: The result of the explanation.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">attn_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">collect_attention_map</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">rollout</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rollout</span><span class="p">(</span><span class="o">*</span><span class="n">attn_maps</span><span class="p">)</span>

        <span class="c1"># attn btw cls and patches</span>
        <span class="n">attrs</span> <span class="o">=</span> <span class="n">rollout</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span>
        <span class="n">n_patches</span> <span class="o">=</span> <span class="n">attrs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>        
        <span class="n">bsz</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="n">p_h</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">h</span> <span class="o">/</span> <span class="n">w</span> <span class="o">*</span> <span class="n">n_patches</span> <span class="o">**</span> <span class="mf">.5</span><span class="p">)</span>
        <span class="n">p_w</span> <span class="o">=</span> <span class="n">n_patches</span> <span class="o">//</span> <span class="n">p_h</span>
        <span class="n">attrs</span> <span class="o">=</span> <span class="n">attrs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">p_h</span><span class="p">,</span> <span class="n">p_w</span><span class="p">)</span>

        <span class="c1"># upsampling</span>
        <span class="n">attrs</span> <span class="o">=</span> <span class="n">LayerAttribution</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span>
            <span class="n">layer_attribution</span><span class="o">=</span><span class="n">attrs</span><span class="p">,</span>
            <span class="n">interpolate_dims</span><span class="o">=</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">),</span>
            <span class="n">interpolate_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">interpolate_mode</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">attrs</span>

    <span class="k">def</span> <span class="nf">get_tunables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Provides Tunable parameters for the optimizer</span>

<span class="sd">        Tunable parameters:</span>
<span class="sd">            `interpolate_mode` (str): Value can be selected of `&quot;bilinear&quot;` and `&quot;bicubic&quot;`</span>

<span class="sd">            `head_fusion_method` (str): Value can be selected of `&quot;min&quot;`, `&quot;max&quot;`, and `&quot;mean&quot;`</span>

<span class="sd">            `discard_ratio` (float): Value can be selected in the range of `range(0, 0.95, 0.05)`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;interpolate_mode&#39;</span><span class="p">:</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;choices&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="s1">&#39;bicubic&#39;</span><span class="p">]}),</span>
            <span class="s1">&#39;head_fusion_method&#39;</span><span class="p">:</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;choices&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="s1">&#39;max&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">]}),</span>
            <span class="s1">&#39;discard_ratio&#39;</span><span class="p">:</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;low&#39;</span><span class="p">:</span> <span class="mf">0.</span><span class="p">,</span> <span class="s1">&#39;high&#39;</span><span class="p">:</span> <span class="mf">.95</span><span class="p">,</span> <span class="s1">&#39;step&#39;</span><span class="p">:</span> <span class="mf">.05</span><span class="p">}),</span>
        <span class="p">}</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="pnpxai.explainers.attention_rollout.AttentionRolloutBase.attribute" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">attribute</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>Computes attributions for the given inputs and targets.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>inputs</code></td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The input data.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>targets</code></td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The target labels for the inputs.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="torch.Tensor">Tensor</span>, <span title="typing.Tuple">Tuple</span>[<span title="torch.Tensor">Tensor</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>torch.Tensor: The result of the explanation.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>pnpxai/explainers/attention_rollout.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">attribute</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">inputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">targets</span><span class="p">:</span> <span class="n">Tensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes attributions for the given inputs and targets.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (torch.Tensor): The input data.</span>
<span class="sd">        targets (torch.Tensor): The target labels for the inputs.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: The result of the explanation.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">attn_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">collect_attention_map</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">rollout</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rollout</span><span class="p">(</span><span class="o">*</span><span class="n">attn_maps</span><span class="p">)</span>

    <span class="c1"># attn btw cls and patches</span>
    <span class="n">attrs</span> <span class="o">=</span> <span class="n">rollout</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span>
    <span class="n">n_patches</span> <span class="o">=</span> <span class="n">attrs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>        
    <span class="n">bsz</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="n">p_h</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">h</span> <span class="o">/</span> <span class="n">w</span> <span class="o">*</span> <span class="n">n_patches</span> <span class="o">**</span> <span class="mf">.5</span><span class="p">)</span>
    <span class="n">p_w</span> <span class="o">=</span> <span class="n">n_patches</span> <span class="o">//</span> <span class="n">p_h</span>
    <span class="n">attrs</span> <span class="o">=</span> <span class="n">attrs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">p_h</span><span class="p">,</span> <span class="n">p_w</span><span class="p">)</span>

    <span class="c1"># upsampling</span>
    <span class="n">attrs</span> <span class="o">=</span> <span class="n">LayerAttribution</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span>
        <span class="n">layer_attribution</span><span class="o">=</span><span class="n">attrs</span><span class="p">,</span>
        <span class="n">interpolate_dims</span><span class="o">=</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">),</span>
        <span class="n">interpolate_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">interpolate_mode</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">attrs</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="pnpxai.explainers.attention_rollout.AttentionRolloutBase.get_tunables" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get_tunables</span><span class="p">()</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>Provides Tunable parameters for the optimizer</p>


<details class="tunable-parameters" open>
  <summary>Tunable parameters</summary>
  <p><code>interpolate_mode</code> (str): Value can be selected of <code>"bilinear"</code> and <code>"bicubic"</code></p>
<p><code>head_fusion_method</code> (str): Value can be selected of <code>"min"</code>, <code>"max"</code>, and <code>"mean"</code></p>
<p><code>discard_ratio</code> (float): Value can be selected in the range of <code>range(0, 0.95, 0.05)</code></p>
</details>
            <details class="quote">
              <summary>Source code in <code>pnpxai/explainers/attention_rollout.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_tunables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Provides Tunable parameters for the optimizer</span>

<span class="sd">    Tunable parameters:</span>
<span class="sd">        `interpolate_mode` (str): Value can be selected of `&quot;bilinear&quot;` and `&quot;bicubic&quot;`</span>

<span class="sd">        `head_fusion_method` (str): Value can be selected of `&quot;min&quot;`, `&quot;max&quot;`, and `&quot;mean&quot;`</span>

<span class="sd">        `discard_ratio` (float): Value can be selected in the range of `range(0, 0.95, 0.05)`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;interpolate_mode&#39;</span><span class="p">:</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;choices&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="s1">&#39;bicubic&#39;</span><span class="p">]}),</span>
        <span class="s1">&#39;head_fusion_method&#39;</span><span class="p">:</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;choices&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="s1">&#39;max&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">]}),</span>
        <span class="s1">&#39;discard_ratio&#39;</span><span class="p">:</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;low&#39;</span><span class="p">:</span> <span class="mf">0.</span><span class="p">,</span> <span class="s1">&#39;high&#39;</span><span class="p">:</span> <span class="mf">.95</span><span class="p">,</span> <span class="s1">&#39;step&#39;</span><span class="p">:</span> <span class="mf">.05</span><span class="p">}),</span>
    <span class="p">}</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pnpxai.explainers.attention_rollout.TransformerAttribution" class="doc doc-heading">
            <code>TransformerAttribution</code>


</h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="pnpxai.explainers.attention_rollout.AttentionRolloutBase" href="#pnpxai.explainers.attention_rollout.AttentionRolloutBase">AttentionRolloutBase</a></code></p>


      <p>Implementation of <code>TransformerAttribution</code> explainer.</p>
<p>Supported Modules: <code>Attention</code></p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>model</code></td>
            <td>
                  <code><span title="torch.nn.modules.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The PyTorch model for which attribution is to be computed.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>interpolate_mode</code></td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The interpolation mode used by the explainer. Available methods are: "bilinear" and "bicubic"</p>
              </div>
            </td>
            <td>
                  <code>&#39;bilinear&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>head_fusion_method</code></td>
            <td>
                  <code><span title="typing.Literal">Literal</span>[&#39;min&#39;, &#39;max&#39;, &#39;mean&#39;]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>(Optional[str]): Method to apply to head fusion. Available methods are: <code>"min"</code>, <code>"max"</code>, <code>"mean"</code></p>
              </div>
            </td>
            <td>
                  <code>&#39;mean&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>discard_ratio</code></td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>(Optional[float]): Describes ration of attention values to discard.</p>
              </div>
            </td>
            <td>
                  <code>0.9</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>forward_arg_extractor</code></td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Callable">Callable</span>[[<span title="typing.Tuple">Tuple</span>[<span title="torch.Tensor">Tensor</span>]], <span title="typing.Union">Union</span>[<span title="torch.Tensor">Tensor</span>, <span title="typing.Tuple">Tuple</span>[<span title="torch.Tensor">Tensor</span>]]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Optional function to extract forward arguments from inputs.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>additional_forward_arg_extractor</code></td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Callable">Callable</span>[[<span title="typing.Tuple">Tuple</span>[<span title="torch.Tensor">Tensor</span>]], <span title="typing.Union">Union</span>[<span title="torch.Tensor">Tensor</span>, <span title="typing.Tuple">Tuple</span>[<span title="torch.Tensor">Tensor</span>]]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Optional function to extract additional forward arguments.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>n_classes</code></td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>(Optional[int]): Number of classes</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>**kwargs</code></td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Keyword arguments that are forwarded to the base implementation of the Explainer</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


<details class="reference" open>
  <summary>Reference</summary>
  <p>Chefer H., Gur S., and Wolf L. Self-Attention Attribution: Transformer interpretability beyond attention visualization.</p>
</details>
              <details class="quote">
                <summary>Source code in <code>pnpxai/explainers/attention_rollout.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">TransformerAttribution</span><span class="p">(</span><span class="n">AttentionRolloutBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implementation of `TransformerAttribution` explainer.</span>

<span class="sd">    Supported Modules: `Attention`</span>

<span class="sd">    Parameters:</span>
<span class="sd">        model (Module): The PyTorch model for which attribution is to be computed.</span>
<span class="sd">        interpolate_mode (Optional[str]): The interpolation mode used by the explainer. Available methods are: &quot;bilinear&quot; and &quot;bicubic&quot;</span>
<span class="sd">        head_fusion_method: (Optional[str]): Method to apply to head fusion. Available methods are: `&quot;min&quot;`, `&quot;max&quot;`, `&quot;mean&quot;`</span>
<span class="sd">        discard_ratio: (Optional[float]): Describes ration of attention values to discard.</span>
<span class="sd">        forward_arg_extractor (Optional[Callable[[Tuple[Tensor]], Union[Tensor, Tuple[Tensor]]]]): Optional function to extract forward arguments from inputs.</span>
<span class="sd">        additional_forward_arg_extractor (Optional[Callable[[Tuple[Tensor]], Union[Tensor, Tuple[Tensor]]]]): Optional function to extract additional forward arguments.</span>
<span class="sd">        n_classes: (Optional[int]): Number of classes</span>
<span class="sd">        **kwargs: Keyword arguments that are forwarded to the base implementation of the Explainer</span>

<span class="sd">    Reference:</span>
<span class="sd">        Chefer H., Gur S., and Wolf L. Self-Attention Attribution: Transformer interpretability beyond attention visualization.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">SUPPORTED_MODULES</span> <span class="o">=</span> <span class="p">[</span><span class="n">Attention</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span>
        <span class="n">interpolate_mode</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s1">&#39;bilinear&#39;</span><span class="p">]</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span>
        <span class="n">head_fusion_method</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="s1">&#39;max&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">]</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span>
        <span class="n">discard_ratio</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span>
        <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span>
        <span class="n">stabilizer</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span>
        <span class="n">zennit_canonizers</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Canonizer</span><span class="p">]]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Module</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Module</span><span class="p">]]]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">forward_arg_extractor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ForwardArgumentExtractor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">additional_forward_arg_extractor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ForwardArgumentExtractor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">n_classes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span>
            <span class="n">interpolate_mode</span><span class="p">,</span>
            <span class="n">head_fusion_method</span><span class="p">,</span>
            <span class="n">discard_ratio</span><span class="p">,</span>
            <span class="n">forward_arg_extractor</span><span class="p">,</span>
            <span class="n">additional_forward_arg_extractor</span><span class="p">,</span>
            <span class="n">n_classes</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stabilizer</span> <span class="o">=</span> <span class="n">stabilizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">zennit_canonizers</span> <span class="o">=</span> <span class="n">zennit_canonizers</span> <span class="ow">or</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">layer</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">default_head_fusion_fn</span><span class="p">(</span><span class="n">attns</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">attns</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">zennit_composite</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">layer_map</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">(</span><span class="n">MultiheadAttention</span><span class="p">,</span> <span class="n">CGWAttentionPropagation</span><span class="p">(</span>
                <span class="n">alpha</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span>
                <span class="n">beta</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span>
                <span class="n">stabilizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilizer</span><span class="p">,</span>
                <span class="n">save_attn_output_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)),</span>
            <span class="p">(</span><span class="n">Linear</span><span class="p">,</span> <span class="n">AlphaBeta</span><span class="p">(</span>
                <span class="n">alpha</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span>
                <span class="n">beta</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span>
                <span class="n">stabilizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilizer</span><span class="p">,</span>
            <span class="p">)),</span>
        <span class="p">]</span> <span class="o">+</span> <span class="n">layer_map_base</span><span class="p">(</span><span class="n">stabilizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilizer</span><span class="p">)</span>
        <span class="n">canonizers</span> <span class="o">=</span> <span class="n">default_attention_converters</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">zennit_canonizers</span>
        <span class="k">return</span> <span class="n">LayerMapComposite</span><span class="p">(</span><span class="n">layer_map</span><span class="o">=</span><span class="n">layer_map</span><span class="p">,</span> <span class="n">canonizers</span><span class="o">=</span><span class="n">canonizers</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_layer_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LayerGradient</span><span class="p">:</span>
        <span class="n">wrapped_model</span> <span class="o">=</span> <span class="n">captum_wrap_model_input</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">wrapped_model</span><span class="o">.</span><span class="n">input_maps</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
            <span class="k">else</span> <span class="n">layer</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer</span>
        <span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer</span>
        <span class="k">return</span> <span class="n">LayerGradient</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">wrapped_model</span><span class="p">,</span>
            <span class="n">layer</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span>
            <span class="n">composite</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">zennit_composite</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Gradient</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Gradient</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
            <span class="n">composite</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">zennit_composite</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">attributor</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layer_gradient</span>

    <span class="k">def</span> <span class="nf">collect_attention_map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
        <span class="n">forward_args</span><span class="p">,</span> <span class="n">additional_forward_args</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_forward_args</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">attributor</span> <span class="k">as</span> <span class="n">attributor</span><span class="p">:</span>
            <span class="n">attributor</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
                <span class="n">forward_args</span><span class="o">=</span><span class="n">forward_args</span><span class="p">,</span>
                <span class="n">targets</span><span class="o">=</span><span class="n">targets</span><span class="p">,</span>
                <span class="n">additional_forward_args</span><span class="o">=</span><span class="n">additional_forward_args</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">grads</span><span class="p">,</span> <span class="n">rels</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">hook_ref</span> <span class="ow">in</span> <span class="n">attributor</span><span class="o">.</span><span class="n">composite</span><span class="o">.</span><span class="n">hook_refs</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">hook_ref</span><span class="p">,</span> <span class="n">CGWAttentionPropagation</span><span class="p">):</span>
                    <span class="n">grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">hook_ref</span><span class="o">.</span><span class="n">stored_tensors</span><span class="p">[</span><span class="s2">&quot;attn_grads&quot;</span><span class="p">])</span>
                    <span class="n">rels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">hook_ref</span><span class="o">.</span><span class="n">stored_tensors</span><span class="p">[</span><span class="s2">&quot;attn_rels&quot;</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">grads</span><span class="p">,</span> <span class="n">rels</span>

    <span class="k">def</span> <span class="nf">rollout</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">rels</span><span class="p">):</span>
        <span class="n">bsz</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">assert</span> <span class="n">tgt_len</span> <span class="o">==</span> <span class="n">src_len</span><span class="p">,</span> <span class="s2">&quot;Must be self-attention&quot;</span>
        <span class="n">rollout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">tgt_len</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">grad</span><span class="p">,</span> <span class="n">rel</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">rels</span><span class="p">):</span>
            <span class="n">grad_x_rel</span> <span class="o">=</span> <span class="n">grad</span> <span class="o">*</span> <span class="n">rel</span>
            <span class="n">attn_map</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_fusion_function</span><span class="p">(</span><span class="n">grad_x_rel</span><span class="p">)</span>
            <span class="n">attn_map</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_discard</span><span class="p">(</span><span class="n">attn_map</span><span class="p">)</span>
            <span class="n">identity</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">tgt_len</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">attn_map</span> <span class="o">=</span> <span class="mf">.5</span> <span class="o">*</span> <span class="n">attn_map</span> <span class="o">+</span> <span class="mf">.5</span> <span class="o">*</span> <span class="n">identity</span>
            <span class="n">attn_map</span> <span class="o">/=</span> <span class="n">attn_map</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">rollout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">rollout</span><span class="p">,</span> <span class="n">attn_map</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">rollout</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>




  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": [], "search": "../../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.af256bd8.min.js"></script>
      
    
  </body>
</html>