{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluator\n",
    "\n",
    "The Evaluator module provides several metrics to evaluate the result explainations generated by `explainers`. Based on Co-12 properties [1], this module provides several metrics to compute the score for each property. \n",
    "\n",
    "## Properties of evaluation\n",
    "| Property [1] | Explanation | Corresponding Metrics | Reference |\n",
    "| --- | --- | --- | --- |\n",
    "| Correctness | Evaluates the truth/reliability of the explanation of the prediction model (AI model). In other words, it indicates how truthful the explanation is compared to the behavior of the black box model. | Mufidelity | [2] |\n",
    "| Continuity | Evaluates the degree to which a prediction model (AI model) is explained. Providing “the whole truth” of a black box model has a high degree of completeness, but a good description must balance compactness and correctness. | Sensitivity | [3] |\n",
    "| Compactness | Evaluates the size/amount of explanation. Ensure that you do not present complex and redundant explanations that are difficult to understand. | Complexity | [2] |\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pnpxai.utils import set_seed\n",
    "from pnpxai.explainers import LRP, ExplainerWArgs\n",
    "from pnpxai.evaluator import XaiEvaluator\n",
    "from pnpxai.evaluator.mu_fidelity import MuFidelity\n",
    "from pnpxai.evaluator.sensitivity import Sensitivity\n",
    "from pnpxai.evaluator.complexity import Complexity\n",
    "\n",
    "from helpers import get_imagenet_dataset, get_torchvision_model\n",
    "\n",
    "set_seed(seed=0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load model, dataset, and explainer\n",
    "model, transform = get_torchvision_model(\"resnet18\")\n",
    "model = model.to(device)\n",
    "explainer = ExplainerWArgs(\n",
    "    explainer=LRP(model=model),\n",
    "    kwargs={\"epsilon\": 1e-6, \"n_classes\": 1000},\n",
    ")\n",
    "\n",
    "dataset = get_imagenet_dataset(transform=transform, subset_size=8)\n",
    "loader = DataLoader(dataset, batch_size=8)\n",
    "inputs, targets = next(iter(loader))\n",
    "inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "# make explanation\n",
    "attrs = explainer.attribute(inputs, targets)\n",
    "\n",
    "# test evaluator\n",
    "metrics = [MuFidelity(n_perturbations=10), Sensitivity(n_iter=10), Complexity()]\n",
    "evaluator = XaiEvaluator(metrics=metrics)\n",
    "evaluations = evaluator(inputs, targets, explainer, attrs)\n",
    "```\n",
    "\n",
    "## Reference\n",
    "\n",
    "[1] M. Nauta, J. Trienes, S. Pathak, E. Nguyen, M. Peters, Y. Schmitt, J. Schlötterer, M. V. Keulen, C. Seifert. From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI. ACM Comput. Surv. 55(13s): 295:1-295:42 (2023).\n",
    "\n",
    "[2] U. Bhatt, A. Weller, and J. M. F. Moura. Evaluating and aggregating feature-based model explanations. In Proceedings of the IJCAI (2020).\n",
    "\n",
    "[3] C.-K. Yeh, C.-Y. Hsieh, A.S. Suggala, D.I. Inouye, and P. Ravikumar. On the (in)fidelity and sensitivity of explanations. In Proceedings of the NeurIPS (2019).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
