
from typing import Optional, Any, Callable, Union, Dict, Literal

from tqdm import tqdm
from optuna.trial import Trial, TrialState

from pnpxai.core._types import DataSource
from pnpxai.core.modality.modality import Modality
from pnpxai.explainers import Explainer, KernelShap, Lime
from pnpxai.explainers.utils.postprocess import PostProcessor, Identity
from pnpxai.evaluator.metrics.base import Metric
from pnpxai.utils import format_into_tuple, format_out_tuple_if_single


class Objective:
    """
    A class that encapsulates the logic for evaluating a model's performance using a 
    specified explainer, postprocessor, and metric within a given modality. The `Objective` 
    class is designed to be callable and can be used within an optimization framework 
    like Optuna to evaluate different configurations of explainers and postprocessors.

    Parameters:
        explainer (Explainer): 
            The explainer used to generate attributions for the model.
        postprocessor (PostProcessor): 
            The postprocessor applied to the attributions generated by the explainer.
        metric (Metric): 
            The metric used to evaluate the effectiveness of the postprocessed attributions.
        modality (Modality): 
            The data modality (e.g., image, text) the model and explainer are operating on.
        inputs (Optional[TensorOrTupleOfTensors], optional): 
            The input data to the model. Defaults to None.
        targets (Optional[Tensor], optional): 
            The target labels for the input data. Defaults to None.
    """

    EXPLAINER_KEY = 'explainer'
    POSTPROCESSOR_KEY = 'postprocessor'

    def __init__(
        self,
        modality: Modality,
        explainer: Explainer,
        metric: Metric,
        data: DataSource,
        disable_tunable_params: Optional[Dict[str, Any]] = None,
        target_class_extractor: Optional[Callable[[Any], Any]] = None,
        label_key: Optional[Union[str, int]] = -1,
        target_labels: bool = False,
        show_progress: bool = False,
        errors: Literal['raise', 'ignore'] = 'raise',
    ):
        self.modality = modality
        self.explainer = explainer
        self.metric = metric
        self.data = data
        self.target_class_extractor = target_class_extractor
        self.label_key = label_key
        self.target_labels = target_labels
        self.show_progress = show_progress
        self.disable_tunable_params = disable_tunable_params or {}
        self.disable_tunable_params_()
        self.errors = errors

    def disable_tunable_params_(self):
        if self.explainer.is_tunable():
            for key, value in self.disable_tunable_params.items():
                param = getattr(self.explainer, key)
                param.update_value(value) # fix value
                param.disable() # disable tuning


    def __call__(self, trial: Trial) -> float:
        """
        Executes the objective function within an optimization trial. This method is 
        responsible for selecting the explainer and postprocessor based on the trial 
        suggestions, performing the explanation and postprocessing, and evaluating the 
        results using the specified metric.

        Parameters:
            trial (Trial): 
                The trial object from an optimization framework like Optuna.

        Returns:
            float: The evaluation result of the metric after applying the explainer and 
            postprocessor to the model's predictions. Returns `nan` if the postprocessed 
            results contain non-countable values like `nan` or `inf`.
        """
        # suggest explainer
        if self.explainer.__class__.is_tunable():
            suggested_explainer = self.explainer.suggest(trial, key=self.EXPLAINER_KEY)
        else:
            suggested_explainer = self.explainer

        # suggest postprocessor
        modalities = format_into_tuple(self.modality)
        suggested_pps = tuple()
        for pp_loc, modality in enumerate(modalities):
            if (
                isinstance(suggested_explainer, (Lime, KernelShap))
                and modality.dtype_key == int
            ):
                modality.util_functions['pooling_fn'].add_fallback_option(
                    key='identity', value=Identity)
                pp_base = PostProcessor(modality, 'identity')
                pp_base.disable_tunable_param('pooling_method')
            else:
                pp_base = PostProcessor(modality)
            suggested_pp = pp_base.suggest(
                trial,
                key=f'{self.POSTPROCESSOR_KEY}.{pp_loc}',
            )
            suggested_pps += (suggested_pp,)

        '''
        Although the number of trials is larger than the number of search grids,
        the number of actual trials will be limited to the number of search grids
        by the following exception.
        '''
        # ignore duplicated samples
        states_to_consider = (TrialState.COMPLETE,)
        trials_to_consider = trial.study.get_trials(
            deepcopy=False,
            states=states_to_consider,
        )
        for t in reversed(trials_to_consider):
            if trial.params == t.params:
                return t.value

        # actual trial
        evals_sum = 0.
        pbar = self.data
        if self.show_progress:
            pbar = tqdm(pbar, total=len(pbar))
        for batch in pbar:
            inputs = suggested_explainer._wrapped_model.extract_inputs(batch)
            if self.target_labels:
                targets = batch[self.label_key]
            else:
                formatted = suggested_explainer._wrapped_model.format_inputs(inputs)
                outputs = suggested_explainer._wrapped_model(*formatted)
                targets = self.target_class_extractor(outputs)
            try:
                attrs = format_into_tuple(suggested_explainer.attribute(inputs, targets))
            except Exception as e:
                if self.errors == 'raise':
                    raise e
                return float('nan')
            attrs_pp = tuple()
            for pp, attr in zip(suggested_pps, attrs):
                attr_pp = pp(attr)
                if any(a.isnan().sum() > 0 or a.isinf().sum() > 0 for a in attr_pp):
                    return float('nan')
                attrs_pp += (attr_pp,)
            attrs_pp = format_out_tuple_if_single(attrs_pp)
            evals = self.metric.set_explainer(suggested_explainer).evaluate(
                inputs, targets, attrs_pp,
            )
            for ev in format_into_tuple(evals):
                evals_sum += ev.sum().item()
        return evals_sum / len(self.data.dataset)
